---
bibliography: benchmarking.bib
---

# Benchmarking AI {#sec-benchmarking_ai}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-benchmarking-ai-resource), [Videos](#sec-benchmarking-ai-resource), [Exercises](#sec-benchmarking-ai-resource)
:::

![_DALLÂ·E 3 Prompt: Photo of a podium set against a tech-themed backdrop. On each tier of the podium, there are AI chips with intricate designs. The top chip has a gold medal hanging from it, the second one has a silver medal, and the third has a bronze medal. Banners with 'AI Olympics' are displayed prominently in the background._](images/png/cover_ai_benchmarking.png)

## Purpose {.unnumbered}

_How can quantitative evaluation reshape the development of machine learning systems, and what metrics reveal true system capabilities?_

The measurement and analysis of AI system performance represent a critical element in bridging theoretical capabilities with practical outcomes. Systematic evaluation approaches reveal fundamental relationships between model behavior, resource utilization, and operational reliability. These measurements draw out the essential trade-offs across accuracy, efficiency, and scalability, providing insights that guide architectural decisions throughout the development lifecycle. These evaluation frameworks establish core principles for assessing and validating system design choices and enable the creation of robust solutions that meet increasingly complex performance requirements across diverse deployment scenarios.

::: {.callout-tip title="Learning Objectives"}

* Understand the purpose and goals of benchmarking AI systems, including performance assessment, resource evaluation, validation, and more.

* Learn about key model benchmarks, metrics, and trends, including accuracy, fairness, complexity, performance, and energy efficiency.

* Become familiar with the key components of an AI benchmark, including datasets, tasks, metrics, baselines, reproducibility rules, and more.

* Understand the distinction between training and inference and how each phase warrants specialized ML systems benchmarking.

* Learn about system benchmarking concepts like throughput, latency, power, and computational efficiency.

* Appreciate the evolution of model benchmarking from accuracy to more holistic metrics like fairness, robustness, and real-world applicability.

* Recognize the growing role of data benchmarking in evaluating issues like bias, noise, balance, and diversity.

* Understand the limitations of evaluating models, data, and systems in isolation and the emerging need for integrated benchmarking.

:::

## Overview

Computing systems continue to evolve and grow in complexity. Understanding their performance becomes essential to engineer them better. System evaluation measures how computing systems perform relative to specified requirements and goals. Engineers and researchers examine metrics like processing speed, resource usage, and reliability to understand system behavior under different conditions and workloads. These measurements help teams identify bottlenecks, optimize performance, and verify that systems meet design specifications.

Standardized measurement forms the backbone of scientific and engineering progress. The metric system enables precise communication of physical quantities. Organizations like the National Institute of Standards and Technology maintain fundamental measures from the kilogram to the second. This standardization extends to computing, where benchmarks provide uniform methods to quantify system performance. Standard performance tests measure processor operations, memory bandwidth, network throughput, and other computing capabilities. These benchmarks allow meaningful comparison between different hardware and software configurations.

Machine learning systems present distinct measurement challenges. Unlike traditional computing tasks, ML systems integrate hardware performance, algorithmic behavior, and data characteristics. Performance evaluation must account for computational efficiency and statistical effectiveness. Training time, model accuracy, and generalization capabilities all factor into system assessment. The interdependence between computing resources, algorithmic choices, and dataset properties creates new dimensions for measurement and comparison.

These considerations lead us to define machine learning benchmarking as follows:

::: {.callout-note title="Definition of ML Benchmarking"}  

**Machine Learning Benchmarking (ML Benchmarking)** is the _systematic evaluation_ of _compute performance, algorithmic effectiveness, and data quality_ in machine learning systems. It assesses _system capabilities_, _model accuracy and convergence_, and _data scalability and representativeness_ to optimize system performance across diverse workloads. ML benchmarking enables engineers and researchers to _quantify trade-offs_, _improve deployment efficiency_, and _ensure reproducibility_ in both research and production settings. As ML systems evolve, benchmarks also incorporate _fairness, robustness, and energy efficiency_, reflecting the increasing complexity of AI evaluation.  

:::  

This chapter focuses primarily on benchmarking machine learning systems, examining how computational resources affect training and inference performance. While the main emphasis remains on system-level evaluation, understanding the role of algorithms and data proves essential for comprehensive ML benchmarking.

## Historical Context

The evolution of computing benchmarks mirrors the development of computer systems themselves, progressing from simple performance metrics to increasingly specialized evaluation frameworks. As computing expanded beyond scientific calculations into diverse applications, benchmarks evolved to measure new capabilities, constraints, and use cases. This progression reflects three major shifts in computing: the transition from mainframes to personal computers, the rise of energy efficiency as a critical concern and the emergence of specialized computing domains such as machine learning.

Early benchmarks focused primarily on raw computational power, measuring basic operations like floating-point calculations. As computing applications diversified, benchmark development branched into distinct specialized categories; each designed to evaluate specific aspects of system performance. This specialization accelerated with the emergence of graphics processing, mobile computing, and eventually, cloud services and artificial intelligence.

### Performance Benchmarks

The evolution of benchmarks in computing illustrates how systematic performance measurement has shaped technological progress. During the 1960s and 1970s, when mainframe computers dominated the computing landscape, performance benchmarks focused primarily on fundamental computational tasks. The [Whetstone benchmark](https://en.wikipedia.org/wiki/Whetstone_(benchmark)), introduced in 1972 to measure floating-point arithmetic performance, became a definitive standard that demonstrated how systematic testing could drive improvements in computer architecture.

The 1980s marked a transformative shift with the emergence of personal computing. The intense competition among manufacturers like IBM, Apple, and Commodore necessitated objective performance metrics. The [SPEC CPU benchmarks](https://www.spec.org/cpu/), introduced in 1989 by the [System Performance Evaluation Cooperative (SPEC)](https://www.spec.org/), fundamentally changed hardware evaluation by introducing a standardized suite that emphasized real-world applications over synthetic tests. This approach enabled manufacturers to optimize their systems for practical workloads, significantly accelerating the pace of [CPU development](https://www.spec.org/benchmarks.html).

The 1990s and early 2000s brought new benchmarking challenges with the rise of graphics-intensive applications and mobile computing. The demand for evaluating graphics performance led to Futuremark's creation of [3DMark](https://www.3dmark.com/). This benchmark's introduction of tests like pixel shader performance measurement in 2001 directly influenced GPU architecture development, driving innovations like programmable shader units in consumer graphics cards. Meanwhile, the proliferation of mobile devices introduced power consumption as a critical metric. Benchmarks like [MobileMark](https://bapco.com/products/mobilemark-2014/) by [BAPCo](https://bapco.com/) began evaluating both speed and battery life, influencing the development of energy-efficient architectures like [ARM](https://www.arm.com/) that prioritized power efficiency while maintaining performance.

The focus of the recent decade has shifted towards cloud computing, big data, and artificial intelligence. Cloud service providers like Amazon Web Services and Google Cloud compete on performance, scalability, and cost-effectiveness. Tailored cloud benchmarks like [CloudSuite](http://cloudsuite.ch/) have become essential, driving providers to optimize their infrastructure for better services.

### Energy Benchmarks

In the evolution of computing performance measurement, energy efficiency emerged as a crucial dimension as systems scaled from personal devices to massive data centers. The mid-2000s marked a pivotal transition in how the industry evaluated computing systems. While early benchmarks focused primarily on raw computational speed, the recognition of power consumption as a critical limiting factor, in both mobile devices and large-scale computing facilities, drove the development of specialized metrics and methodologies.

The impact of energy consumption and environmental considerations has fundamentally transformed how we evaluate computing systems. This transformation gained momentum in the mid-2000s when processors began encountering practical thermal limits, while simultaneously, the scaling demands driven by internet services highlighted energy efficiency as a crucial factor. In modern computing, energy considerations influence system design across all scales, from personal devices to enterprise data centers, making power benchmarking an essential component of system evaluation.

Power benchmarking addresses three interconnected challenges in the computing industry: environmental sustainability, operational efficiency, and device usability. The expanding carbon footprint of the technology sector has elevated environmental impact to a critical concern, while energy costs significantly impact data center economics. In the mobile computing sector, power efficiency directly determines battery life and fundamentally shapes the user experience.

The industry has developed several standardized benchmarks to measure and drive improvements in energy efficiency. [SPEC Power](https://www.spec.org/power/) provides comprehensive server efficiency measurements across varying workload levels, offering a standardized methodology for comparing power-performance trade-offs between different systems. The [Green500](https://www.top500.org/green500/) list applies these efficiency principles to supercomputing, ranking the world's most powerful systems based on their energy efficiency rather than just raw performance. While not a traditional benchmark, the [ENERGY STAR](https://www.energystar.gov/products/computers) certification program has established foundational efficiency standards that have significantly influenced the design of consumer electronics.

Power benchmarking faces distinct challenges, particularly in accounting for diverse workload patterns and system configurations, while accurately measuring power consumption across hardware that ranges from microWatts to megaWatts. Recent developments, such as [MLPerf](https://mlcommons.org/), have introduced specialized power measurement methodologies for AI workloads, acknowledging the substantial energy impact of machine learning operations in modern data centers. As artificial intelligence and edge computing continue their rapid expansion, power benchmarking will likely become increasingly critical, driving innovation in energy-efficient hardware design and software optimization techniques.

### Domain-Specific Benchmarks

The evolution of computing applications, particularly in the AI domain, exposed inherent limitations in standardized benchmarks, catalyzing the next phase in performance evaluation: domain-specific frameworks. Organizations recognized that generic benchmarks often failed to capture critical aspects of specialized workloads, leading to the development of targeted evaluation methodologies tailored to distinct operational domains.

Beyond industry-standard metrics, organizations now develop domain-specific benchmarks that precisely align with their particular field of operation and use cases. These specialized frameworks enable detailed performance evaluation that directly corresponds to real-world applications, uncovering critical insights that standardized benchmarks might overlook. This tailored approach has become particularly vital in AI systems, where requirements can vary dramatically across different sectors.

Domain-specific benchmarks address diverse industry requirements through carefully crafted evaluation scenarios. Healthcare organizations might develop benchmarks that assess patient readmission prediction models using metrics specifically calibrated to their patient population characteristics and historical medical data patterns. In the financial sector, organizations often create specialized frameworks that evaluate the delicate balance between fraud detection accuracy and the stringent timing requirements of transaction processing. Autonomous vehicle developers implement domain-focused benchmarks that assess AI system performance across a comprehensive matrix of environmental conditions, traffic patterns, and safety-critical parameters.

The distinctive advantage of domain-specific benchmarks lies in their flexibility and precision. Organizations can engineer test suites that specifically target performance characteristics crucial to their sector, such as the microsecond-level latency requirements in high-frequency trading platforms or the strict memory constraints in edge computing devices. This focused evaluation approach enables the identification of potential performance issues and optimization opportunities that standard benchmarks might fail to surface.

The development of domain-specific benchmarks requires careful consideration of multiple critical elements: operational data that accurately represents the target environment, specific performance limitations of the deployment infrastructure, and distinct quality metrics that align with sector requirements. These specialized evaluation frameworks typically complement industry-standard benchmarks, providing a comprehensive assessment that captures both general performance characteristics and domain-specific capabilities within the intended operational context.

## AI Benchmarks: System, Model, and Data

## AI Benchmarks: System, Model, and Data  

The evolution of benchmarks reaches its apex in machine learning, reflecting a journey that parallels the field's development towards domain-specific applications. Early machine learning benchmarks focused primarily on algorithmic performance, measuring how well models could perform specific tasks. As machine learning applications scaled and computational demands grew, the focus expanded to include system performance and hardware efficiency. Most recently, the critical role of data quality has emerged as the third essential dimension of evaluation.  

What sets AI benchmarks apart from traditional performance metrics is their inherent variability---introducing accuracy as a fundamental dimension of evaluation. Unlike conventional benchmarks, which measure fixed, deterministic characteristics like computational speed or energy consumption, AI benchmarks must account for the probabilistic nature of machine learning models. The same system can produce different results depending on the data it encounters, making accuracy a defining factor in performance assessment. This distinction adds complexity, as benchmarking AI systems requires not only measuring raw computational efficiency but also understanding trade-offs between accuracy, generalization, and resource constraints.  

The growing complexity and ubiquity of machine learning systems demand comprehensive benchmarking across all three dimensions: algorithmic models, hardware systems, and training data. This multifaceted evaluation approach represents a significant departure from earlier benchmarks that could focus on isolated aspects like computational speed or energy efficiency. Modern machine learning benchmarks must address the sophisticated interplay between these dimensions, as limitations in any one area can fundamentally constrain overall system performance.  

This evolution in benchmark complexity mirrors the field's deepening understanding of what drives machine learning system success. While algorithmic innovations initially dominated progress metrics, the challenges of deploying models at scale revealed the critical importance of hardware efficiency. Subsequently, high-profile failures of machine learning systems in real-world deployments highlighted how data quality and representation fundamentally determine system reliability and fairness. Understanding how these dimensions interact has become essential for accurately assessing machine learning system performance, informing development decisions, and measuring technological progress in the field.

### Algorithmic Benchmarks

AI algorithms must balance multiple interconnected performance objectives, including accuracy, speed, resource efficiency, and generalization capability. As machine learning applications span diverse domainsâsuch as computer vision, natural language processing, speech recognition, and reinforcement learningâevaluating these objectives requires standardized methodologies tailored to each domain's unique challenges. Algorithmic benchmarks establish these evaluation frameworks, providing a consistent basis for comparing different machine learning approaches.

::: {.callout-note title="Definition of Machine Learning  Algorithmic Benchmarks"}

**ML Algorithmic benchmarks** refer to the evaluation of machine learning models on _standardized tasks_ using _predefined datasets and metrics_. These benchmarks measure _accuracy, efficiency, and generalization_ to ensure _objective comparisons_ across different models. Algorithmic benchmarks provide _performance baselines_, enabling systematic assessment of _trade-offs between model complexity and computational cost_. They drive _technological progress_ by tracking improvements over time and identifying _limitations_ in existing approaches.

:::

Algorithmic benchmarks serve several critical functions in advancing AI. They establish clear performance baselines, enabling objective comparisons between competing approaches. By systematically evaluating trade-offs between model complexity, computational requirements, and task performance, they help researchers and practitioners identify optimal design choices. Moreover, they track technological progress by documenting improvements over time, guiding the development of new techniques while exposing limitations in existing methodologies. Through these roles, algorithmic benchmarks shape the trajectory of AI research and development, ensuring that innovations translate into measurable, real-world improvements.

### System Benchmarks

AI computations, particularly in machine learning, place extraordinary demands on computational resources. The underlying hardware infrastructure, encompassing CPUs, GPUs, TPUs, and specialized accelerators, fundamentally determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish standardized methodologies for evaluating hardware performance across diverse AI workloads, measuring critical metrics including computational throughput, memory bandwidth, power efficiency, and scaling characteristics.  

::: {.callout-note title="Definition of Machine Learning System Benchmarks"}  

**ML System benchmarks** refer to the evaluation of _computational infrastructure_ used to execute AI workloads, assessing _performance, efficiency, and scalability_ under standardized conditions. These benchmarks measure _throughput, latency, and resource utilization_ to ensure _objective comparisons_ across different system configurations. System benchmarks provide _insights into workload efficiency_, guiding _infrastructure selection, system optimization,_ and _advancements in computational architectures_.  

:::  

These benchmarks fulfill two essential functions in the AI ecosystem. First, they enable developers and organizations to make informed decisions when selecting hardware platforms for their AI applications by providing comprehensive comparative performance data across system configurations. Critical evaluation factors include training speed, inference latency, energy efficiency, and cost-effectiveness. Second, hardware manufacturers rely on these benchmarks to quantify generational improvements and guide the development of specialized AI accelerators, driving continuous advancement in computational capabilities.  

System benchmarks evaluate performance across multiple scales, ranging from single-chip configurations to large distributed systems, and diverse AI workloads including both training and inference tasks. This comprehensive evaluation approach ensures that benchmarks accurately reflect real-world deployment scenarios and deliver actionable insights that inform both hardware selection decisions and system architecture design.  

### Data Benchmarks  

Data quality, scale, and diversity fundamentally shape machine learning system performance, directly influencing how effectively algorithms learn and generalize to new situations. Data benchmarks establish standardized datasets and evaluation methodologies that enable consistent comparison of different approaches. These frameworks assess critical aspects of data quality, including domain coverage, potential biases, and resilience to real-world variations in input data.  

::: {.callout-note title="Definition of Machine Learning Data Benchmarks"}  

**ML Data benchmarks** refer to the evaluation of _datasets and data quality_ in machine learning, assessing _coverage, bias, and robustness_ under standardized conditions. These benchmarks measure _data representativeness, consistency, and impact on model performance_ to ensure _objective comparisons_ across different AI approaches. Data benchmarks provide _insights into data reliability_, guiding _dataset selection, bias mitigation,_ and _improvements in data-driven AI systems_.  

:::  

Data benchmarks serve an essential function in understanding AI system behavior under diverse data conditions. Through systematic evaluation, they help identify common failure modes, expose gaps in data coverage, and reveal underlying biases that could impact model behavior in deployment. By providing common frameworks for data evaluation, these benchmarks enable the AI community to systematically improve data quality and address potential issues before deploying systems in production environments. This proactive approach to data quality assessment has become increasingly critical as AI systems take on more complex and consequential tasks across different domains.  

### Community Consensus

The proliferation of benchmarks spanning performance, energy efficiency, and domain-specific applications creates a fundamental challenge: establishing industry-wide standards. While early computing benchmarks primarily measured processor speed and memory bandwidth, modern benchmarks evaluate sophisticated aspects of system performance, from power consumption profiles to application-specific capabilities. This evolution in scope and complexity necessitates comprehensive validation and consensus from the computing community, particularly in rapidly evolving fields like machine learning where performance must be evaluated across multiple interdependent dimensions.

The lasting impact of a benchmark depends fundamentally on its acceptance by the research community, where technical excellence alone proves insufficient. Benchmarks developed without broad community input often fail to gain traction, frequently missing metrics that leading research groups consider essential. Successful benchmarks emerge through collaborative development involving academic institutions, industry partners, and domain experts. This inclusive approach ensures benchmarks evaluate capabilities most crucial for advancing the field, while balancing theoretical and practical considerations.

Benchmarks developed through extensive collaboration among respected institutions carry the authority necessary to drive widespread adoption, while those perceived as advancing particular corporate interests face skepticism and limited acceptance. The success of ImageNet demonstrates how sustained community engagement through workshops and challenges establishes long-term viability. This community-driven development creates a foundation for formal standardization, where organizations like IEEE and ISO transform these benchmarks into official standards.

The standardization process provides crucial infrastructure for benchmark formalization and adoption. [IEEE working groups](https://standards.ieee.org/develop/wg/) transform community-developed benchmarking methodologies into formal industry standards, establishing precise specifications for measurement and reporting. The [IEEE 2416-2019](https://standards.ieee.org/ieee/2416/7065/) standard for system performance measurement exemplifies this process, codifying best practices developed through community consensus. Similarly, [ISO/IEC technical committees](https://www.iso.org/committee/45020.html) develop international standards for benchmark validation and certification, ensuring consistent evaluation across global research and industry communities. These organizations bridge the gap between community-driven innovation and formal standardization, providing frameworks that enable reliable comparison of results across different institutions and geographic regions.

Successful community benchmarks establish clear governance structures for managing their evolution. Through rigorous version control systems and detailed change documentation, benchmarks maintain backward compatibility while incorporating new advances. This governance includes formal processes for proposing, reviewing, and implementing changes, ensuring that benchmarks remain relevant while maintaining stability. Modern benchmarks increasingly emphasize reproducibility requirements, incorporating automated verification systems and standardized evaluation environments.

Open access accelerates benchmark adoption and ensures consistent implementation. Projects that provide open-source reference implementations, comprehensive documentation, validation suites, and containerized evaluation environments reduce barriers to entry. This standardization enables research groups to evaluate solutions using uniform methods and metrics. Without such coordinated implementation frameworks, organizations might interpret benchmarks inconsistently, compromising result reproducibility and meaningful comparison across studies.

The most successful benchmarks strike a careful balance between academic rigor and industry practicality. Academic involvement ensures theoretical soundness and comprehensive evaluation methodology, while industry participation grounds benchmarks in practical constraints and real-world applications. This balance proves particularly crucial in machine learning benchmarks, where theoretical advances must translate to practical improvements in deployed systems.

Community consensus establishes enduring benchmark relevance, while fragmentation impedes scientific progress. Through collaborative development and transparent operation, benchmarks evolve into authoritative standards for measuring advancement. The most successful benchmarks in energy efficiency and domain-specific applications share this foundation of community development and governance, demonstrating how collective expertise and shared purpose create lasting impact in rapidly advancing fields.

## Benchmark Components

An AI benchmark is more than just a test or a score; it is a comprehensive evaluation framework designed to assess the performance and capabilities of artificial intelligence systems. To understand this in-depth, it is essential to break down the typical components that constitute an AI benchmark.

### Standardized Datasets

Standardized datasets form the cornerstone of AI benchmarks, providing a consistent foundation for training and evaluating models. These datasets are meticulously curated to ensure that all models are tested on equal footing, allowing for meaningful comparisons across different approaches and architectures. In the field of computer vision, datasets such as [ImageNet](http://www.image-net.org/), [COCO](https://cocodataset.org/), and [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) have become gold standards. For natural language processing, corpora such as [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), [GLUE](https://gluebenchmark.com/), and [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) serve similar roles. These datasets often encompass a wide range of complexities and edge cases, pushing machine learning systems to their limits and revealing their true capabilities.

The choice of dataset significantly influences benchmark results, making it crucial to select datasets that accurately represent real-world challenges. Moreover, as machine learning systems evolve, so too must these datasets to continue providing relevant and challenging evaluation scenarios.

### Pre-defined Tasks

Pre-defined tasks are essential components of machine learning benchmarks, providing clear objectives against which models can be evaluated. These tasks simulate real-world problems and challenges, allowing researchers and practitioners to assess a model's practical capabilities.

Tasks are typically designed to test specific aspects of machine learning systems, such as classification, regression, generation, or reasoning. For instance, in natural language processing, common tasks include sentiment analysis, machine translation, and question answering. In computer vision, object detection, image segmentation, and facial recognition are frequently used benchmark tasks.

The complexity and diversity of these tasks play a crucial role in the benchmark's effectiveness. Well-designed tasks should:

1. Reflect real-world applications
2. Cover a range of difficulties
3. Test multiple aspects of model performance
4. Be clearly defined and unambiguous

As machine learning advances, benchmark tasks must evolve to keep pace with emerging capabilities and challenges, ensuring they remain relevant and informative.

The revised section on Evaluation Metrics maintains a textbook tone while providing a comprehensive overview of the topic. The flow is logical, starting with a general definition and importance of evaluation metrics, then moving to specific types of metrics for different tasks, and finally addressing more advanced considerations.

### Evaluation Metrics

Evaluation metrics are quantitative measures used to assess the performance of machine learning models on specific tasks. These metrics provide objective standards for comparing different models and approaches, enabling researchers and practitioners to gauge the effectiveness of their solutions. The selection of appropriate evaluation metrics is a critical aspect of benchmark design. Metrics must align closely with the task objectives and provide meaningful insights into model performance. For classification tasks, common metrics include accuracy, precision, recall, and F1 score. These metrics offer different perspectives on a model's ability to correctly identify and categorize data points.

In regression tasks, where the goal is to predict continuous values, error-based metrics are typically employed. Mean Squared Error (MSE) and Mean Absolute Error (MAE) quantify the discrepancy between predicted and actual values, providing a measure of the model's predictive accuracy. Certain domains require specialized metrics to capture task-specific nuances. In natural language processing, for instance, the BLEU score is frequently used to evaluate machine translation quality, assessing the similarity between machine-generated translations and those produced by human experts.

It is important to note that commonly used metrics may not always capture the full complexity of model performance. For example, in imbalanced datasets, accuracy can be misleading. Researchers must consider the limitations of each metric and select those most appropriate for their specific task and data distribution. As machine learning models become more sophisticated and are increasingly deployed in real-world applications, additional performance aspects have gained prominence. Metrics related to inference time, model size, and energy efficiency are now often incorporated into benchmarks. These metrics address practical considerations of model deployment, reflecting the growing need for solutions that balance accuracy with computational efficiency.

No single metric can fully capture all aspects of model performance. Consequently, comprehensive benchmarks often employ a combination of metrics to provide a more holistic evaluation. This multi-faceted approach allows for a nuanced understanding of a model's strengths and limitations across various performance dimensions.

### Baselines and Baseline Models

Baselines and baseline models serve as fundamental reference points in AI benchmarks. These standard models or methods provide a basis for comparison, allowing researchers to contextualize the performance of more advanced techniques. In benchmark design, the selection of appropriate baselines is crucial. Simple models, such as linear regression for continuous predictions or logistic regression for classification tasks, often serve as initial baselines. These straightforward approaches establish a minimum performance threshold that more complex models should surpass. More sophisticated baselines may include established machine learning algorithms or architectures that have demonstrated strong performance on similar tasks. For instance, in natural language processing tasks, models like BERT or GPT might serve as baselines against which newer models are compared.

The inclusion of multiple baselines in a benchmark offers several advantages. Baselines provide a frame of reference for interpreting the results of more advanced models. By comparing new models to established baselines, researchers can quantify improvements in the field over time. Simple baselines can highlight cases where complex models may be unnecessarily sophisticated for a given task. Additionally, comparing advanced models to baselines across various datasets can reveal potential overfitting or task-specific optimizations.

It is important to note that as the field progresses, baselines that were once considered state-of-the-art may become outdated. Consequently, benchmark designers must periodically reassess and update their baseline models to ensure they remain relevant and challenging. By providing these points of comparison, baselines and baseline models play a critical role in the rigorous evaluation of machine learning techniques, contributing to the overall advancement of the field.

### Hardware and Software Specifications

Hardware and software specifications are critical components of machine learning benchmarks, as they significantly influence model performance, training time, and reproducibility of results. These specifications provide a detailed description of the computational environment in which experiments are conducted.

Hardware specifications typically include:

1. Processor type and speed (e.g., CPU model, clock rate)
2. Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs), if used
3. Memory capacity and type (e.g., RAM size, DDR4)
4. Storage type and capacity (e.g., SSD, HDD)
5. Network configuration, if relevant for distributed computing

Software specifications generally encompass:

1. Operating system and version
2. Programming language and version
3. Machine learning frameworks and libraries (e.g., TensorFlow, PyTorch) with version numbers
4. Compiler information and optimization flags
5. Custom software or scripts used in the benchmark process

The precise documentation of these specifications is crucial for several reasons. Firstly, it ensures reproducibility, allowing other researchers to replicate the benchmark environment as closely as possible. Secondly, it provides context for interpreting performance metrics, as hardware capabilities can significantly impact training and inference times. Lastly, it helps in understanding the scalability and resource requirements of different models.

In many cases, benchmarks may include results from multiple hardware configurations to provide a more comprehensive view of model performance across different computational environments. This approach is particularly valuable as it highlights the trade-offs between model complexity, computational resources, and performance.

As the field evolves, hardware and software specifications in benchmarks are increasingly including information about energy consumption and computational efficiency. This trend reflects growing concerns about the environmental impact of large-scale machine learning models and the need for sustainable AI practices.

By clearly defining hardware and software specifications, benchmarks contribute to the transparency and reliability of machine learning research, facilitating fair comparisons and fostering progress in the field.

### Environmental Conditions

Environmental conditions in AI benchmarking refer to the physical and operational circumstances under which experiments are conducted. These conditions, while often overlooked, can significantly influence benchmark results and impact the reproducibility of experiments. Physical environmental factors include ambient temperature, humidity, air quality, and altitude. These elements can affect hardware performance in subtle but measurable ways. For instance, elevated temperatures may lead to thermal throttling in processors, potentially reducing computational speed and affecting benchmark outcomes. Similarly, variations in altitude can impact cooling system efficiency and hard drive performance due to changes in air pressure.

Operational environmental factors encompass the broader system context in which benchmarks are executed. This includes background processes running on the system, network conditions, and power supply stability. The presence of other active programs or services can compete for computational resources, potentially altering the performance characteristics of the model under evaluation. To ensure the validity and reproducibility of benchmark results, it is crucial to document and control these environmental conditions to the extent possible. This may involve conducting experiments in temperature-controlled environments, monitoring and reporting ambient conditions, standardizing the operational state of benchmark systems, and documenting any background processes or system loads.

In scenarios where controlling all environmental variables is impractical, such as in distributed or cloud-based benchmarking, it becomes essential to report these conditions in detail. This information allows other researchers to account for potential variations when interpreting or attempting to reproduce results. As machine learning models are increasingly deployed in diverse real-world environments, understanding the impact of environmental conditions on model performance becomes even more critical. This knowledge not only ensures more accurate benchmarking but also informs the development of robust models capable of consistent performance across varying operational conditions.

### Reproducibility Rules

Reproducibility is a cornerstone of scientific research, and AI benchmarks are no exception. Reproducibility rules are guidelines and practices that ensure benchmark results can be reliably replicated by other researchers or practitioners. These rules are crucial for validating claims, building upon existing work, and advancing the field of machine learning. At the heart of reproducibility in AI benchmarks is the concept of controlled randomness. Many machine learning algorithms involve stochastic processes, such as random weight initialization or data shuffling. To address this, benchmarks often specify seed values for random number generators. By using the same seed, researchers can ensure that the "random" elements of their experiments are consistent across different runs and environments.

Another key aspect of reproducibility is the complete documentation of hyperparameters. These are the settings that control the learning process but are not learned from the data itself. Examples include learning rates, batch sizes, and network architectures. Even small changes in hyperparameters can lead to significant differences in model performance, making their precise documentation essential. Benchmarks also typically require the preservation and sharing of training and evaluation datasets. In cases where data cannot be directly shared due to privacy or licensing concerns, benchmarks may provide detailed descriptions of data preprocessing steps and selection criteria. This allows other researchers to assemble comparable datasets or understand the characteristics of the data used in the original experiments.

Code availability is another critical component of reproducibility rules. Many benchmarks now require the publication of implementation code, preferably in a version-controlled repository. This code should include not only the model implementation but also scripts for data preprocessing, training, and evaluation. Some benchmarks go a step further, providing containerized environments that encapsulate all necessary dependencies and configurations. Detailed logging of the training process and intermediate results is also often mandated. This includes recording metrics at regular intervals during training, saving model checkpoints, and documenting any manual interventions or adjustments made during the experiment.

By adhering to these reproducibility rules, AI benchmarks foster transparency, facilitate peer review, and accelerate scientific progress. They allow the community to verify results, build upon successful approaches, and identify potential issues or limitations in proposed methods. As the field of machine learning continues to evolve rapidly, the importance of robust reproducibility practices in benchmarking cannot be overstated.

### Result Interpretation Guidelines

Result interpretation guidelines are an essential component of AI benchmarks, providing a framework for understanding and contextualizing benchmark outcomes. These guidelines help researchers and practitioners draw meaningful conclusions from benchmark results, ensuring fair and informative comparisons between different models or approaches. A key aspect of result interpretation is understanding the statistical significance of performance differences. Benchmarks often specify guidelines for conducting appropriate statistical tests and reporting confidence intervals. This approach helps distinguish between meaningful improvements and variations that may be due to random chance or noise in the data.

Benchmarks typically provide context for interpreting metrics in light of real-world applications. For instance, a small improvement in accuracy might be considered significant for certain high-stakes applications, while in other contexts, factors like inference speed or model size might take precedence over marginal accuracy gains. It is crucial for users to understand the limitations of the benchmark itself. No benchmark can cover all possible use cases or scenarios. Guidelines often address where benchmark results are most applicable and where they might not generalize well, including discussions of dataset biases, task-specific quirks, or limitations in the evaluation metrics.

Many benchmarks now include guidelines for interpreting results across different performance axes. For example, a model might excel in accuracy but perform poorly in terms of computational efficiency. Interpretation guidelines help users navigate these trade-offs and select models that best fit their specific requirements and constraints. The issue of overfitting to the benchmark is frequently addressed in these guidelines. As benchmarks become widely used, there's a risk that researchers might optimize their models specifically for the benchmark tasks, potentially at the expense of generalizability. Guidelines may suggest ways to assess a model's performance beyond the specific benchmark tasks or caution against over-interpreting small performance gains.

Cross-validation techniques play a crucial role in result interpretation. By evaluating models on multiple subsets of the data, researchers can gain a more robust understanding of model performance and reduce the risk of overfitting to a particular dataset split. Benchmark guidelines often specify cross-validation procedures to ensure consistent and reliable performance estimates. Result interpretation guidelines often encourage a holistic view of model performance. This may include considering factors beyond the primary metrics, such as model robustness, fairness across different demographic groups, or performance degradation over time.

By providing these interpretation guidelines, benchmarks not only offer a standardized way to measure performance but also promote a nuanced and contextualized understanding of machine learning model capabilities. This approach supports more informed decision-making in both research and practical applications of machine learning technologies.

Standardized datasets are the cornerstone of machine learning benchmarks, providing a consistent foundation for training and evaluation. These carefully curated collections of data ensure that all models are tested on equal footing, allowing for meaningful comparisons across different approaches and architectures.

In the field of computer vision, datasets like [ImageNet](http://www.image-net.org/), [COCO](https://cocodataset.org/), and [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) have become gold standards. For natural language processing, corpora such as [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), [GLUE](https://gluebenchmark.com/), and [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) serve similar roles. These datasets often encompass a wide range of complexities and edge cases, pushing machine learning systems to their limits and revealing their true capabilities.

The choice of dataset significantly influences benchmark results, making it crucial to select datasets that accurately represent real-world challenges. Moreover, as machine learning systems evolve, so too must these datasets to continue providing relevant and challenging evaluation scenarios.

## Benchmarking Granularity

Machine learning system benchmarking provides a structured and systematic approach to assessing a system's performance across various dimensions. Given the complexity of ML systems, we can dissect their performance through different levels of granularity and obtain a comprehensive view of the system's efficiency, identify potential bottlenecks, and pinpoint areas for improvement. To this end, various types of benchmarks have evolved over the years and continue to persist.

@fig-granularity shows the different layers of granularity of an ML system. At the application level, end-to-end benchmarks assess the overall system performance, considering factors like data preprocessing, model training, and inference. While at the model layer, benchmarks focus on assessing the efficiency and accuracy of specific models. This includes evaluating how well models generalize to new data and their computational efficiency during training and inference. Furthermore, benchmarking can extend to hardware and software infrastructure, examining the performance of individual components like GPUs or TPUs.

![ML system granularity.](images/png/end2end.png){#fig-granularity}

### Micro Benchmarks

Micro-benchmarks are specialized evaluation tools that assess distinct components or specific operations within a broader machine learning process. These benchmarks isolate individual tasks to provide detailed insights into the computational demands of particular system elements, from neural network layers to optimization techniques to activation functions. For example, micro-benchmarks might measure the time required to execute a convolutional layer in a deep learning model or evaluate the speed of data preprocessing operations that prepare training data.

A key area of micro-benchmarking focuses on tensor operations, which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn) by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix multiplications across different hardware configurations. These measurements help developers understand how their hardware handles the core mathematical operations that dominate ML workloads.

Micro-benchmarks also examine activation functions and neural network layers in isolation. This includes measuring the performance of various activation functions like ReLU, Sigmoid, and Tanh under controlled conditions, as well as evaluating the computational efficiency of distinct neural network components such as LSTM cells or Transformer blocks when processing standardized inputs.

[DeepBench](https://github.com/baidu-research/DeepBench), developed by Baidu, was one of the first to demonstrate the value of comprehensive micro-benchmarking. It evaluates these fundamental operations across different hardware platforms, providing detailed performance data that helps developers optimize their deep learning implementations. By isolating and measuring individual operations, DeepBench enables precise comparison of hardware platforms and identification of potential performance bottlenecks.

:::{#exr-cuda .callout-caution collapse="true" title="Benchmarking Tensor Operations"}

Ever wonder how your image filters get so fast? Special libraries like cuDNN supercharge those calculations on certain hardware. In this Colab, we'll use cuDNN with PyTorch to speed up image filtering. Think of it as a tiny benchmark, showing how the right software can unlock your GPU's power!  
  
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/RyanHartzell/cudnn-image-filtering/blob/master/notebooks/CuDNN%20Image%20Filtering%20Tutorial%20Using%20PyTorch.ipynb#scrollTo=1sWeXdYsATrr)

:::

### Macro Benchmarks

While micro-benchmarks examine individual operations like tensor computations and layer performance, macro benchmarks evaluate complete machine learning models. This shift from component-level to model-level assessment provides insights into how architectural choices and component interactions affect overall model behavior. For instance, while micro-benchmarks might show optimal performance for individual convolutional layers, macro-benchmarks reveal how these layers work together within a complete convolutional neural network.

Macro-benchmarks measure multiple performance dimensions that emerge only at the model level. These include prediction accuracy, which shows how well the model generalizes to new data; memory consumption patterns across different batch sizes and sequence lengths; throughput under varying computational loads; and latency across different hardware configurations. Understanding these metrics helps developers make informed decisions about model architecture, optimization strategies, and deployment configurations.

The assessment of complete models occurs under standardized conditions using established datasets and tasks. For example, computer vision models might be evaluated on [ImageNet](https://www.image-net.org/), measuring both computational efficiency and prediction accuracy. Natural language processing models might be assessed on translation tasks, examining how they balance quality and speed across different language pairs.

Several industry-standard benchmarks enable consistent model evaluation across platforms. [MLPerf Inference](https://github.com/mlcommons/inference) provides comprehensive testing suites adapted for different computational environments. [MLPerf Mobile](https://github.com/mlcommons/mobile_app_open) focuses on mobile device constraints, while [MLPerf Tiny](https://github.com/mlcommons/tiny) addresses microcontroller deployments. For embedded systems, [EEMBC's MLMark](https://github.com/eembc/mlmark) emphasizes both performance and power efficiency. The [AI-Benchmark](https://ai-benchmark.com/) suite specializes in mobile platforms, evaluating models across diverse tasks from image recognition to face parsing.

### End-to-end Benchmarks

End-to-end benchmarks provide an all-inclusive evaluation that extends beyond the boundaries of the ML model itself. Rather than focusing solely on a machine learning model's computational efficiency or accuracy, these benchmarks encompass the entire pipeline of an AI system. This includes initial ETL (Extract-Transform-Load) or ELT (Extract-Load-Transform) data processing, the core model's performance, post-processing of results, and critical infrastructure components like storage and network systems.

Data processing is the foundation of all AI systems, transforming raw data into a format suitable for model training or inference. In ETL pipelines, data undergoes extraction from source systems, transformation through cleaning and feature engineering, and loading into model-ready formats. These preprocessing steps' efficiency, scalability, and accuracy significantly impact overall system performance. End-to-end benchmarks must assess standardized datasets through these pipelines to ensure data preparation doesn't become a bottleneck.

The post-processing phase plays an equally important role. This involves interpreting the model's raw outputs, converting scores into meaningful categories, filtering results based on predefined tasks, or integrating with other systems. For instance, a computer vision system might need to post-process detection boundaries, apply confidence thresholds, and format results for downstream applications. In real-world deployments, this phase proves crucial for delivering actionable insights.

Beyond core AI operations, infrastructure components heavily influence overall performance and user experience. Storage solutions, whether cloud-based, on-premises, or hybrid, can significantly impact data retrieval and storage times, especially with vast AI datasets. Network interactions, vital for distributed systems, can become performance bottlenecks if not optimized. End-to-end benchmarks must evaluate these components under specified environmental conditions to ensure reproducible measurements of the entire system.

To date, there are no public, end-to-end benchmarks that fully account for data storage, network, and compute performance. While MLPerf Training and Inference approach end-to-end evaluation, they primarily focus on model performance rather than real-world deployment scenarios. Nonetheless, they provide valuable baseline metrics for assessing AI system capabilities.

Given the inherent specificity of end-to-end benchmarking, organizations typically perform these evaluations internally by instrumenting production deployments. This allows engineers to develop result interpretation guidelines based on realistic workloads, but given the sensitivity and specificity of the information, these benchmarks rarely appear in public settings.

### Understanding the Trade-offs

As shown in @tbl-benchmark-comparison, different challenges emerge at different stages of an AI system's lifecycle. Each benchmarking approach provides unique insights: micro-benchmarks help engineers optimize specific components like GPU kernel implementations or data loading operations, macro-benchmarks guide model architecture decisions and algorithm selection, while end-to-end benchmarks reveal system-level bottlenecks in production environments.

+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+
| Component   | Micro Benchmarks                                          | Macro Benchmarks                                       | End-to-End Benchmarks                                  |
+:============+:==========================================================+:=======================================================+:=======================================================+
| Focus       | Individual operations                                     | Complete models                                        | Full system pipeline                                   |
+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+
| Scope       | Tensor ops, layers, activations                           | Model architecture, training, inference                | ETL, model, infrastructure                             |
+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+
| Example     | Conv layer performance on cuDNN                           | ResNet-50 on ImageNet                                  | Production recommendation system                       |
+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+
| Advantages  | Precise bottleneck identification, Component optimization | Model architecture comparison, Standardized evaluation | Realistic performance assessment, System-wide insights |
+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+
| Challenges  | May miss interaction effects                              | Limited infrastructure insights                        | Complex to standardize, Often proprietary              |
+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+
| Typical Use | Hardware selection, Operation optimization                | Model selection, Research comparison                   | Production system evaluation                           |
+-------------+-----------------------------------------------------------+--------------------------------------------------------+--------------------------------------------------------+

: Comparison of benchmarking approaches across different dimensions. Each approach offers distinct advantages and focuses on different aspects of ML system evaluation. {#tbl-benchmark-comparison .striped .hover}

Component interaction often produces unexpected behaviors. For example, while micro-benchmarks might show excellent performance for individual convolutional layers, and macro-benchmarks might demonstrate strong accuracy for the complete model, end-to-end evaluation could reveal that data preprocessing creates unexpected bottlenecks during high-traffic periods. These system-level insights often remain hidden when components undergo isolated testing.

Component interaction often produces unexpected behaviors. For example, while micro-benchmarks might show excellent performance for individual convolutional layers, and macro-benchmarks might demonstrate strong accuracy for the complete model, end-to-end evaluation could reveal that data preprocessing creates unexpected bottlenecks during high-traffic periods. These system-level insights often remain hidden when components undergo isolated testing.

## Training Benchmarks  

Training benchmarks provide a systematic approach to evaluating the efficiency, scalability, and resource demands of the training phase. They allow practitioners to assess how different design choicesâsuch as model architectures, data loading mechanisms, hardware configurations, and distributed training strategiesâimpact performance. These benchmarks are particularly vital as machine learning systems grow in scale, requiring billions of parameters, terabytes of data, and distributed computing environments.  

For instance, large-scale models like [OpenAI's GPT-3](https://arxiv.org/abs/2005.14165) [@brown2020language], which consists of 175 billion parameters trained on 45 terabytes of data, highlight the immense computational demands of training. Benchmarks enable systematic evaluation of the underlying systems to ensure that hardware and software configurations can meet these demands efficiently.  

::: {.callout-note title="Definition of ML Training Benchmarks"}  

**ML Training Benchmarks** are standardized tools used to evaluate the _performance_, _efficiency_, and _scalability_ of machine learning systems during the _training phase_. These benchmarks measure key _system-level metrics_, such as _time-to-accuracy_, _throughput_, _resource utilization_, and _energy consumption_. By providing a structured evaluation framework, training benchmarks enable _fair comparisons_ across _hardware platforms_, _software frameworks_, and _distributed computing setups_. They help identify _bottlenecks_ and optimize _training processes_ for _large-scale machine learning models_, ensuring that computational resources are used effectively.  

:::  

Efficient data storage and delivery during training also play a major role in the training process. For instance, in a machine learning model that predicts bounding boxes around objects in an image, thousands of images may be required. However, loading an entire image dataset into memory is typically infeasible, so practitioners rely on data loaders from ML frameworks. Successful model training depends on timely and efficient data delivery, making it essential to benchmark tools like data pipelines, preprocessing speed, and storage retrieval times to understand their impact on training performance.  

Hardware selection is another key factor in training machine learning systems, as it can significantly impact training time. Training benchmarks evaluate CPU, GPU, memory, and network utilization during the training phase to guide system optimizations. Understanding how resources are used is essential: Are GPUs being fully leveraged? Is there unnecessary memory overhead? Benchmarks can uncover bottlenecks or inefficiencies in resource utilization, leading to cost savings and performance improvements.  

In many cases, using a single hardware accelerator, such as a single GPU, is insufficient to meet the computational demands of large-scale model training. Machine learning models are often trained in data centers with multiple GPUs or TPUs, where distributed computing enables parallel processing across nodes. Training benchmarks assess how efficiently the system scales across multiple nodes, manages data sharding, and handles challenges like node failures or drop-offs during training.  

To illustrate these benchmarking principles, we will reference MLPerf Training throughout this section. Briefly, MLPerf is an industry-standard benchmark suite designed to evaluate machine learning system performance. It provides standardized tests for training and inference across a range of deep learning workloads, including image classification, language modeling, object detection, and recommendation systems. A full discussion of MLPerf's structure and methodology is presented later in this chapter.

### Purpose  

From a systems perspective, training machine learning models is a computationally intensive process that requires careful optimization of resources. Training benchmarks serve as essential tools for evaluating system efficiency, identifying bottlenecks, and ensuring that machine learning systems can scale effectively. They provide a standardized approach to measuring how various system componentsâsuch as hardware accelerators, memory, storage, and network infrastructureâaffect training performance. By systematically evaluating these factors, training benchmarks enable researchers and engineers to optimize configurations, improve scalability, and reduce overall resource consumption.  

#### Why Training Benchmarks Matter for ML Systems

As machine learning models grow in complexity, training becomes increasingly demanding in terms of compute power, memory, and data storage. The ability to measure and compare training efficiency is critical to ensuring that systems can effectively handle large-scale workloads. Training benchmarks provide a structured methodology for assessing performance across different hardware platforms, software frameworks, and optimization techniques.  

One of the fundamental challenges in training machine learning models is the efficient allocation of computational resources. Training a transformer-based model such as GPT-3, which consists of 175 billion parameters and requires processing terabytes of data, places an enormous burden on modern computing infrastructure. Without standardized benchmarks, it becomes difficult to determine whether a system is fully utilizing its resources or whether inefficienciesâsuch as slow data loading, underutilized accelerators, or excessive memory overheadâare limiting performance.  

Training benchmarks help uncover such inefficiencies by measuring key performance indicators, including system throughput, time-to-accuracy, and hardware utilization. These benchmarks allow practitioners to analyze whether GPUs, TPUs, and CPUs are being leveraged effectively or whether specific bottlenecks, such as memory bandwidth constraints or inefficient data pipelines, are reducing overall system performance. By providing insights into these factors, benchmarks support the design of more efficient training workflows that maximize hardware potential while minimizing unnecessary computation.  

#### Optimizing Hardware & Software Configurations

The performance of machine learning training is heavily influenced by the choice of hardware and software. Training benchmarks guide system designers in selecting optimal configurations by measuring how different architecturesâsuch as GPUs, TPUs, and emerging AI acceleratorsâhandle computational workloads. These benchmarks also evaluate how well deep learning frameworks, such as TensorFlow and PyTorch, optimize performance across different hardware setups.  

For example, the MLPerf Training benchmark suite is widely used to compare the performance of different accelerator architectures on tasks such as image classification, natural language processing, and recommendation systems. By running standardized benchmarks across multiple hardware configurations, engineers can determine whether certain accelerators are better suited for specific training workloads. This information is particularly valuable in large-scale data centers and cloud computing environments, where selecting the right combination of hardware and software can lead to significant performance gains and cost savings.  

Beyond hardware selection, training benchmarks also inform software optimizations. Machine learning frameworks implement various low-level optimizationsâsuch as mixed-precision training, memory-efficient data loading, and distributed training strategiesâthat can significantly impact system performance. Benchmarks help quantify the impact of these optimizations, ensuring that training systems are configured for maximum efficiency.  

#### Scalability & Efficiency

As machine learning workloads continue to grow, efficient scaling across distributed computing environments has become a key concern. Many modern deep learning models are trained across multiple GPUs or TPUs, requiring efficient parallelization strategies to ensure that additional computing resources lead to meaningful performance improvements. Training benchmarks measure how well a system scales by evaluating system throughput, memory efficiency, and overall training time as additional computational resources are introduced.  

Effective scaling is not always guaranteed. While adding more GPUs or TPUs should, in theory, reduce training time, issues such as communication overhead, data synchronization latency, and memory bottlenecks can limit scaling efficiency. Training benchmarks help identify these challenges by quantifying how performance scales with increasing hardware resources. A well-designed system should exhibit near-linear scaling, where doubling the number of GPUs results in a near-halving of training time. However, real-world inefficiencies often prevent perfect scaling, and benchmarks provide the necessary insights to optimize system design accordingly.  

Another crucial factor in training efficiency is time-to-accuracy, which measures how quickly a model reaches a target accuracy level. Achieving faster convergence with fewer computational resources is a key goal in training optimization, and benchmarks help compare different training methodologies to determine which approaches strike the best balance between speed and accuracy. By leveraging training benchmarks, system designers can assess whether their infrastructure is capable of handling large-scale workloads efficiently while maintaining training stability and accuracy.  

#### Cost & Energy Considerations

The computational cost of training large-scale models has risen sharply in recent years, making cost-efficiency a critical consideration. Training a model such as GPT-3 can require millions of dollars in cloud computing resources, making it imperative to evaluate cost-effectiveness across different hardware and software configurations. Training benchmarks provide a means to quantify the cost per training run by analyzing computational expenses, cloud pricing models, and energy consumption.  

Beyond financial cost, energy efficiency has become an increasingly important metric. Large-scale training runs consume vast amounts of electricity, contributing to significant carbon emissions. Benchmarks help evaluate energy efficiency by measuring power consumption per unit of training progress, allowing organizations to identify sustainable approaches to AI development.  

For example, MLPerf includes an energy benchmarking component that tracks the power consumption of various hardware accelerators during training. This allows researchers to compare different computing platforms not only in terms of raw performance but also in terms of their environmental impact. By integrating energy efficiency metrics into benchmarking studies, organizations can design AI systems that balance computational power with sustainability goals.  

#### Fair Comparisons Across ML Systems  

One of the primary functions of training benchmarks is to establish a standardized framework for comparing ML systems. Given the wide variety of hardware architectures, deep learning frameworks, and optimization techniques available today, ensuring fair and reproducible comparisons is essential.  

Standardized benchmarks provide a common evaluation methodology, allowing researchers and practitioners to assess how different training systems perform under identical conditions. For example, MLPerf Training benchmarks enable vendor-neutral comparisons by defining strict evaluation criteria for deep learning tasks such as image classification, language modeling, and recommendation systems. This ensures that performance results are meaningful and not skewed by differences in dataset preprocessing, hyperparameter tuning, or implementation details.  

Furthermore, reproducibility is a major concern in machine learning research. Training benchmarks help address this challenge by providing clearly defined methodologies for performance evaluation, ensuring that results can be consistently reproduced across different computing environments. By adhering to standardized benchmarks, researchers can make informed decisions when selecting hardware, software, and training methodologies, ultimately driving progress in AI systems development.  

### Metrics

Evaluating the performance of machine learning training requires a set of well-defined metrics that go beyond conventional algorithmic measures. From a systems perspective, training benchmarks assess how efficiently and effectively a machine learning model can be trained to a predefined accuracy threshold. Metrics such as throughput, scalability, and energy efficiency are only meaningful in relation to whether the model successfully reaches its target accuracy. Without this constraint, optimizing for raw speed or resource utilization may lead to misleading conclusions.  

Training benchmarks, such as MLPerf Training, define specific accuracy targets for different machine learning tasks, ensuring that performance measurements are made in a fair and reproducible manner. A system that trains a model quickly but fails to reach the required accuracy is not considered a valid benchmark result. Conversely, a system that achieves the best possible accuracy but takes an excessive amount of time or resources may not be practically useful. Effective benchmarking requires balancing speed, efficiency, and accuracy convergence.  

#### Training Time and Throughput

One of the fundamental metrics for evaluating training efficiency is the time required to reach a predefined accuracy threshold. Training time measures how long a model takes to converge to an acceptable performance level, reflecting the overall computational efficiency of the system. Throughput, often expressed as the number of training samples processed per second, provides an additional measure of system performance. However, throughput alone does not guarantee meaningful results, as a model may process a large number of samples quickly without necessarily reaching the desired accuracy.  

For example, in MLPerf Training, the benchmark for ResNet-50 may require reaching an accuracy target like 75.9% top-1 on the ImageNet dataset. A system that processes 10,000 images per second but fails to achieve this accuracy is not considered a valid benchmark result, while a system that processes fewer images per second but converges efficiently is preferable. This highlights why throughput must always be evaluated in relation to time-to-accuracy rather than as an independent performance measure.  

#### Scalability and Parallelism

As machine learning models increase in size, training workloads often require distributed computing across multiple processors or accelerators. Scalability measures how effectively training performance improves as more computational resources are added. An ideal system should exhibit near-linear scaling, where doubling the number of GPUs or TPUs leads to a proportional reduction in training time. However, real-world performance is often constrained by factors such as communication overhead, memory bandwidth limitations, and inefficiencies in parallelization strategies.  

When training large-scale models such as GPT-3, OpenAI employed thousands of GPUs in a distributed training setup. While increasing the number of GPUs provided more raw computational power, the performance improvements were not perfectly linear due to network communication overhead between nodes. Benchmarks such as MLPerf quantify how well a system scales across multiple GPUs, providing insights into where inefficiencies arise in distributed training.  

Parallelism in training is categorized into data parallelism, model parallelism, and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly used strategy, involves splitting the training dataset across multiple compute nodes. The efficiency of this approach depends on synchronization mechanisms and gradient communication overhead. In contrast, model parallelism partitions the neural network itself, requiring efficient coordination between processors. Benchmarks evaluate how well a system manages these parallelism strategies without degrading accuracy convergence.  

#### Resource Utilization

The efficiency of machine learning training depends not only on speed and scalability but also on how well available hardware resources are utilized. Compute utilization measures the extent to which processing units, such as GPUs or TPUs, are actively engaged during training. Low utilization may indicate bottlenecks in data movement, memory access, or inefficient workload scheduling.  

For instance, when training BERT on a TPU cluster, researchers observed that input pipeline inefficiencies were limiting overall throughput. Although the TPUs had high raw compute power, the system was not keeping them fully utilized due to slow data retrieval from storage. By profiling the resource utilization, engineers identified the bottleneck and optimized the input pipeline using TFRecord and data prefetching, leading to improved performance.  

Memory bandwidth is another critical factor, as deep learning models require frequent access to large volumes of data during training. If memory bandwidth becomes a limiting factor, increasing compute power alone will not improve training speed. Benchmarks assess how well models leverage available memory, ensuring that data transfer rates between storage, main memory, and processing units do not become performance bottlenecks.  

I/O performance also plays a significant role in training efficiency, particularly when working with large datasets that cannot fit entirely in memory. Benchmarks evaluate the efficiency of data loading pipelines, including preprocessing operations, caching mechanisms, and storage retrieval speeds. Systems that fail to optimize data loading can experience significant slowdowns, regardless of computational power.  

#### Energy Efficiency and Cost

Training large-scale machine learning models requires substantial computational resources, leading to significant energy consumption and financial costs. Energy efficiency metrics quantify the power usage of training workloads, helping identify systems that optimize computational efficiency while minimizing energy waste. The increasing focus on sustainability has led to the inclusion of energy-based benchmarks, such as those in MLPerf Training, which measure power consumption per training run.  

Training GPT-3 was estimated to consume 1,287 MWh of electricity, which is comparable to the yearly energy usage of 100 US households. If a system can achieve the same accuracy with fewer training iterations, it directly reduces energy consumption. Energy-aware benchmarks help guide the development of hardware and training strategies that optimize power efficiency while maintaining accuracy targets.  

Cost considerations extend beyond electricity usage to include hardware expenses, cloud computing costs, and infrastructure maintenance. Training benchmarks provide insights into the cost-effectiveness of different hardware and software configurations by measuring training time in relation to resource expenditure. Organizations can use these benchmarks to balance performance and budget constraints when selecting training infrastructure.  

#### Fault Tolerance and Robustness

Training workloads often run for extended periods, sometimes spanning days or weeks, making fault tolerance an essential consideration. A robust system must be capable of handling unexpected failures, including hardware malfunctions, network disruptions, and memory errors, without compromising accuracy convergence.  

In large-scale cloud-based training, node failures are common due to hardware instability. If a GPU node in a distributed cluster fails, training must continue without corrupting the model. MLPerf Training includes evaluations of fault-tolerant training strategies, such as checkpointing, where models periodically save their progress. This ensures that failures do not require restarting the entire training process.  

#### Reproducibility and Standardization

For benchmarks to be meaningful, results must be reproducible across different runs, hardware platforms, and software frameworks. Variability in training results can arise due to stochastic processes, hardware differences, and software optimizations. Ensuring reproducibility requires standardizing evaluation protocols, controlling for randomness in model initialization, and enforcing consistency in dataset processing.  

MLPerf Training enforces strict reproducibility requirements, ensuring that accuracy results remain stable across multiple training runs. When NVIDIA submitted benchmark results for MLPerf, they had to demonstrate that their ResNet-50 ImageNet training time remained consistent across different GPUs. This ensures that benchmarks measure true system performance rather than noise from randomness.  

### Evaluating Training Performance  

There are many different ways to analyze and evaluate system performance in machine learning training. The choice of benchmarking metrics depends on the specific goals of the evaluationâwhether the focus is on optimizing speed, improving resource utilization, enhancing energy efficiency, or ensuring fault tolerance. A well-rounded benchmarking approach must take all these factors into account while ensuring that models reach their intended accuracy targets in a reproducible and scalable manner.  

@tbl-training-metrics provides a structured overview of key system-level training metrics, highlighting different evaluation dimensions and their relevance to training benchmarks. This table serves as a reference for how system performance can be analyzed in the context of machine learning training.  
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| Category                            | Key Metrics                                                                                                            | Example Benchmark Use                                       |
+:====================================+:=======================================================================================================================+:============================================================+
| Training Time and Throughput        | Time-to-accuracy (seconds, minutes, hours); Throughput (samples/sec)                                                   | Comparing training speed across different GPU architectures |
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| Scalability and Parallelism         | Scaling efficiency (% of ideal speedup); Communication overhead (latency, bandwidth)                                   | Analyzing distributed training performance for large models |
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| Resource Utilization                | Compute utilization (% GPU/TPU usage); Memory bandwidth (GB/s); I/O efficiency (data loading speed)                    | Optimizing data pipelines to improve GPU utilization        |
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| Energy Efficiency and Cost          | Energy consumption per run (MWh, kWh); Performance per watt (TOPS/W)                                                   | Evaluating energy-efficient training strategies             |
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| Fault Tolerance and Robustness      | Checkpoint overhead (time per save); Recovery success rate (%)                                                         | Assessing failure recovery in cloud-based training systems  |
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| Reproducibility and Standardization | Variance across runs (% difference in accuracy, training time); Framework consistency (TensorFlow vs. PyTorch vs. JAX) | Ensuring consistency in benchmark results across hardware   |
+-------------------------------------+------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+

: Training benchmark metrics and evaluation dimensions. {#tbl-training-metrics .striped .hover}

#### Common Pitfalls in Training Benchmarks  

Despite the availability of well-defined benchmarking methodologies, certain misconceptions and flawed evaluation practices often lead to misleading conclusions. Understanding these pitfalls is important for interpreting benchmark results correctly.  

##### Focusing only on raw throughput  

A common mistake in training benchmarks is assuming that higher throughput always translates to better training performance. It is possible to artificially increase throughput by using lower numerical precision, reducing synchronization, or even bypassing certain computations. However, these optimizations do not necessarily lead to faster convergence.

For example, a system using TF32 precision may achieve higher throughput than one using FP32, but if TF32 introduces numerical instability that increases the number of iterations required to reach the target accuracy, the overall training time may be longer. The correct way to evaluate throughput is in relation to time-to-accuracy, ensuring that speed optimizations do not come at the expense of convergence efficiency.  

##### Evaluating single-node performance in isolation  

Benchmarking training performance on a single node without considering how well it scales in a distributed setting can lead to misleading conclusions. A GPU may demonstrate excellent throughput when used independently, but when deployed across hundreds of nodes, communication overhead and synchronization constraints may diminish these efficiency gains.  

For instance, a system optimized for single-node performance may employ memory optimizations that do not generalize well to multi-node environments. Large-scale models such as GPT-3 require efficient gradient synchronization across multiple nodes, making it essential to assess scalability rather than relying solely on single-node performance metrics.  

##### Ignoring mid-training failures, fault tolerance, and interference

Many benchmarks assume an idealized training environment where hardware failures, memory corruption, network instability, or interference from other processes do not occur. However, real-world training jobs often experience unexpected failures and workload interference that require checkpointing, recovery mechanisms, and resource management.

A system optimized for ideal-case performance but lacking fault tolerance and interference handling may achieve impressive benchmark results under controlled conditions, but frequent failures, inefficient recovery, and resource contention could make it impractical for large-scale deployment. Effective benchmarking should consider checkpointing overhead, failure recovery efficiency, and the impact of interference from other processes rather than assuming perfect execution conditions.

##### Assuming that scaling efficiency is always linear  

When evaluating distributed training, it is often assumed that increasing the number of GPUs or TPUs will result in proportional speedups. In practice, communication bottlenecks, memory contention, and synchronization overheads lead to diminishing returns as more compute nodes are added.  

For example, training a model across 1,000 GPUs does not necessarily provide 100 times the speed of training on 10 GPUs. At a certain scale, gradient communication costs become a limiting factor, offsetting the benefits of additional parallelism. Proper benchmarking should assess scalability efficiency rather than assuming idealized linear improvements.  

##### Failing to consider reproducibility across frameworks and hardware  

Benchmark results are often reported without verifying their reproducibility across different hardware and software frameworks. Even minor variations in floating-point arithmetic, memory layouts, or optimization strategies can introduce statistical differences in training time and accuracy.  

For example, a benchmark run on TensorFlow with XLA optimizations may exhibit different convergence characteristics compared to the same model trained using PyTorch with Automatic Mixed Precision (AMP). Proper benchmarking requires evaluating results across multiple frameworks to ensure that software-specific optimizations do not distort performance comparisons.  

#### Final Thoughts  

Training benchmarks provide valuable insights into machine learning system performance, but their interpretation requires careful consideration of real-world constraints. High throughput does not necessarily mean faster training if it compromises accuracy convergence. Similarly, scaling efficiency must be evaluated holistically, taking into account both computational efficiency and communication overhead.

By avoiding common benchmarking pitfalls and employing structured evaluation methodologies, machine learning practitioners can gain a deeper understanding of how to optimize training workflows, design efficient AI systems, and develop scalable machine learning infrastructure. As models continue to increase in complexity, benchmarking methodologies must evolve to reflect real-world challenges, ensuring that benchmarks remain meaningful and actionable in guiding AI system development.

### Benchmarks

Benchmarking plays a critical role in evaluating the performance of machine learning training systems. Given the complexity of modern ML workloads, multiple benchmarking efforts have emerged over time, each designed to evaluate different aspects of system performance. Some focus on training efficiency and time-to-accuracy, others on hardware utilization patterns, and some on low-level computational primitives that define deep learning workloads.  

MLPerf Training, one of the most widely used benchmark suites today, evolved from multiple academic and industry efforts that sought to define systematic ways to measure ML training performance. Early efforts such as DAWNBench [@coleman2017dawnbench], Fathom [@adolf2016fathom], and DeepBench [@bai2018deepbench] laid the foundation for understanding and standardizing different dimensions of training performance. These benchmarks each contributed unique insights that continue to shape modern benchmarking practices.  

#### MLPerf Training Benchmark

[MLPerf Training Benchmark](https://github.com/mlcommons/training) is a widely used suite developed by MLCommons to provide a standardized framework for evaluating machine learning system performance. MLPerf emerged as a collaboration between industry and academia, incorporating best practices from previous benchmarking efforts. A key focus of MLPerf is measuring training time to target accuracy, ensuring that system performance is evaluated not just on throughput, but on meaningful model convergence metrics.  

MLPerf Training includes workloads spanning multiple domains, such as image classification, object detection, translation, and reinforcement learning. By requiring benchmarks to report results at a fixed accuracy threshold, MLPerf ensures that comparisons remain valid across different system configurations.  

@fig-perf-trend highlights performance improvements across MLPerf benchmark submissions, demonstrating that machine learning training efficiency has been improving at a rate exceeding Moore's Law [@mattson2020mlperf].  

![MLPerf Training performance trends. Source: @mattson2020mlperf.](images/png/mlperf_perf_trend.png){#fig-perf-trend}  

Key evaluation metrics include:  

* Training time to reach target accuracy  
* Throughput (examples processed per second)  
* Resource utilization (CPU, GPU, memory, disk I/O)  

#### DAWNBench: End-to-End Training Efficiency

[DAWNBench](https://dawn.cs.stanford.edu/benchmark/) was introduced by researchers at Stanford to evaluate end-to-end deep learning training and inference performance. Unlike MLPerf, which focuses on structured evaluation across multiple workloads, DAWNBench emphasized real-world training efficiency by considering the cost and time required to train models in practice.  

DAWNBench helped popularize training time as a first-class metric for ML benchmarking, providing a clear way to compare different hardware, software frameworks, and cloud providers. Its contributions influenced the design of MLPerf by reinforcing the importance of measuring time-to-accuracy rather than just raw speed.  

DAWNBench evaluates:  

* Time-to-train to a target accuracy  
* Inference latency (for assessing deployment efficiency)  
* Cost, including computational and cloud storage expenses  

#### Fathom: Hardware Utilization Patterns

[Fathom](https://github.com/rdadolf/fathom) was developed at Harvard University to analyze deep learning workloads at a systems level, focusing on how different architectures utilize computational resources. While MLPerf and DAWNBench emphasize high-level training efficiency, Fathom provides insights into low-level resource usage, helping identify bottlenecks in hardware-accelerated training.  

Fathom was designed to evaluate:

* Operations per second (measuring computational efficiency)  
* Time to completion for deep learning workloads  
* Memory bandwidth and compute utilization patterns  

By highlighting hardware inefficiencies in deep learning training, Fathom contributed to optimizations in AI accelerators, influencing both academic research and hardware design.  

#### DeepBench: Micro-Benchmarking for Deep Learning

[DeepBench](https://github.com/baidu-research/DeepBench) was introduced by Baidu to analyze the fundamental building blocks of deep learning workloads. Unlike MLPerf or DAWNBench, which evaluate full model training, DeepBench focuses on micro-benchmarking specific operations, such as matrix multiplications and convolutions.  

DeepBench is particularly useful for evaluating the efficiency of:  

* GEMM (General Matrix Multiplication), a core operation in deep learning training  
* Convolutional operations, which dominate CNN workloads  
* Memory access patterns, which affect data movement efficiency in accelerators  

By providing fine-grained insights into the efficiency of key ML operations, DeepBench plays a complementary role to larger benchmarks, helping optimize low-level computational building blocks that influence overall system performance.  

## Case Study: Benchmarking the Training Performance of a Large Language Model  

Benchmarks like MLPerf are often used to compare performance across different hardware platforms, software frameworks, or training configurations. While these comparisons are valuable, the greatest utility of benchmarking lies in its ability to support informed decision-making. By systematically evaluating training efficiency, resource utilization, and scalability, benchmarks help machine learning practitioners identify inefficiencies, optimize training workflows, and allocate computational resources effectively.  

To illustrate how benchmarking translates into actionable insights, consider a research team tasked with training a large transformer-based language model with 175 billion parameters, similar in scale to GPT-3. The model is trained on a massive web-scale dataset containing trillions of tokens, requiring weeks of computation on a high-performance GPU cluster. Given the significant costs associated with such large-scale training, a well-structured benchmarking approach is necessary to ensure that resources are used efficiently.  

### Establishing Benchmarking Objectives  

Before running any benchmarks, the research team outlines the key objectives for evaluating training performance. The primary goal is to determine how long it will take for the model to reach a predefined perplexity threshold, a standard measure of how well a language model predicts unseen text. This metric provides a more meaningful evaluation of training progress than raw throughput alone.  

Additionally, the team seeks to analyze throughput, defined as the number of tokens processed per second, to assess whether the compute resources are being fully utilized. Since large-scale training requires distributed processing across multiple GPUs, the team also evaluates scaling efficiency to determine whether adding more hardware leads to proportional reductions in training time.  

Other important considerations include hardware utilization metrics, such as memory bandwidth usage and GPU compute efficiency, which reveal whether data movement is a bottleneck. Finally, the team assesses energy consumption, given the high computational costs of training large models. Understanding power efficiency will allow them to evaluate the feasibility of different hardware configurations.  

### Executing the Benchmark and Analyzing Results  

With these objectives in mind, the team initiates a benchmarking run on a 128-GPU cluster of NVIDIA H100s, tracking key performance metrics over the course of training. At the end of the run, they collect the following results:  

* Time to reach target perplexity: 12 days  
* Throughput: 50,000 tokens per second  
* GPU utilization: 78 percent  
* Scaling efficiency: 70 percent when increasing from 64 to 128 GPUs  
* Memory bandwidth usage: 82 percent  
* Energy consumption: 2.5 megawatt-hours per day  

The initial results suggest that the system is operating efficiently, but they also highlight areas where further optimizations may be required. For example, GPU utilization is lower than expected, indicating that some computational resources may be idle at certain points in training. Additionally, the scaling efficiency is suboptimal, as increasing the number of GPUs does not yield a proportional speedup in training.  

### Identifying Bottlenecks and Optimizing Performance  

To better understand the observed inefficiencies, the team performs a deeper analysis of system behavior. They first investigate GPU utilization and discover that data loading is creating a bottleneck, preventing full utilization of compute resources. By profiling the training pipeline, they find that data sharding and prefetching mechanisms are not optimized, leading to delays in feeding data to GPUs. To address this, they refine the data pipeline by increasing the number of parallel data-loading threads and pre-processing batches in advance, which results in a 10 percent increase in GPU utilization.  

Next, they examine scaling efficiency and identify gradient synchronization overhead as a limiting factor. In large-scale distributed training, model updates must be synchronized across GPUs, but excessive communication delays can reduce the benefits of parallelism. The team explores tensor parallelism and pipeline parallelism strategies to minimize synchronization delays, ultimately improving scaling efficiency from 70 percent to 85 percent.  

To further improve performance, the team evaluates numerical precision settings. Initially, training was conducted using full 32-bit floating-point precision (FP32), which ensures numerical stability but is computationally expensive. By switching to bfloat16 (BF16) precision, they reduce memory bandwidth requirements and computational overhead, improving throughput without compromising model accuracy.  

Finally, they examine energy consumption trends and find that power efficiency could be improved by dynamic voltage and frequency scaling (DVFS), an adaptive power management technique that adjusts GPU performance levels based on workload demands. By incorporating energy-aware scheduling, they achieve a 5 percent reduction in power usage per training step.  

### Connecting Benchmarking Insights to Real-World Decision-Making  

The results of this benchmarking study provide actionable insights that influence the team's training strategy. By refining data loading, optimizing distributed training efficiency, and adjusting numerical precision, they achieve a significant reduction in training time while maintaining model quality. These optimizations translate to tangible benefits, such as lower cloud computing costs, improved hardware efficiency, and reduced environmental impact.  

Beyond this specific training run, the benchmarking results inform long-term infrastructure planning. The team uses the insights gained to evaluate whether future models would benefit from alternative hardware configurations, such as custom AI accelerators or cloud-based TPU clusters. Additionally, by tracking energy efficiency, they contribute to sustainable AI research efforts, ensuring that computational resources are used responsibly.  

Ultimately, this case study illustrates that benchmarking is not just about comparing hardware performanceâit is a critical tool for guiding optimization strategies, reducing training costs, and improving the overall efficiency of large-scale machine learning systems.

## Inference Benchmarks

Inference in machine learning refers to using a trained model to make predictions on new, unseen data. It is the phase where the model applies its learned knowledge to solve the problem it was designed for, such as classifying images, recognizing speech, or translating text.

### Purpose

When we build machine learning models, our ultimate goal is to deploy them in real-world applications where they can provide accurate and reliable predictions on new, unseen data. This process of using a trained model to make predictions is known as inference. A machine learning model's real-world performance can differ significantly from its performance on training or validation datasets, which makes benchmarking inference a crucial step in the development and deployment of machine learning models.

Benchmarking inference allows us to evaluate how well a machine-learning model performs in real-world scenarios. This evaluation ensures that the model is practical and reliable when deployed in applications, providing a more comprehensive understanding of the model's behavior with real data. Additionally, benchmarking can help identify potential bottlenecks or limitations in the model's performance. For example, if a model takes too long to predict, it may be impractical for real-time applications such as autonomous driving or voice assistants.

Resource efficiency is another critical aspect of inference, as it can be computationally intensive and require significant memory and processing power. Benchmarking helps ensure that the model is efficient regarding resource usage, which is particularly important for edge devices with limited computational capabilities, such as smartphones or IoT devices. Moreover, benchmarking allows us to compare the performance of our model with competing models or previous versions of the same model. This comparison is essential for making informed decisions about which model to deploy in a specific application.

Finally, it is vital to ensure that the model's predictions are not only accurate but also consistent across different data points. Benchmarking helps verify the model's accuracy and consistency, ensuring that it meets the application's requirements. It also assesses the model's robustness, ensuring that it can handle real-world data variability and still make accurate predictions.

### Metrics

1. **Accuracy:** Accuracy is one of the most vital metrics when benchmarking machine learning models. It quantifies the proportion of correct predictions made by the model compared to the true values or labels. For example, if a spam detection model can correctly classify 95 out of 100 email messages as spam or not, its accuracy would be calculated as 95%.

2. **Latency:** Latency is a performance metric that calculates the time lag or delay between the input receipt and the production of the corresponding output by the machine learning system. An example that clearly depicts latency is a real-time translation application; if a half-second delay exists from the moment a user inputs a sentence to the time the app displays the translated text, then the system's latency is 0.5 seconds.

3. **Latency-Bounded Throughput:** Latency-bounded throughput is a valuable metric that combines the aspects of latency and throughput, measuring the maximum throughput of a system while still meeting a specified latency constraint. For example, in a video streaming application that utilizes a machine learning model to generate and display subtitles automatically, latency-bounded throughput would measure how many video frames the system can process per second (throughput) while ensuring that the subtitles are displayed with no more than a 1-second delay (latency). This metric is particularly important in real-time applications where meeting latency requirements is crucial to the user experience.

4. **Throughput:** Throughput assesses the system's capacity by measuring the number of inferences or predictions a machine learning model can handle within a specific unit of time. Consider a speech recognition system that employs a Recurrent Neural Network (RNN) as its underlying model; if this system can process and understand 50 different audio clips in a minute, then its throughput rate stands at 50 clips per minute.

5. **Energy Efficiency:** Energy efficiency is a metric that determines the amount of energy consumed by the machine learning model to perform a single inference. A prime example of this would be a natural language processing model built on a Transformer network architecture; if it utilizes 0.1 Joules of energy to translate a sentence from English to French, its energy efficiency is measured at 0.1 Joules per inference.

6. **Memory Usage:** Memory usage quantifies the volume of RAM needed by a machine learning model to carry out inference tasks. A relevant example to illustrate this would be a face recognition system based on a CNN; if such a system requires 150 MB of RAM to process and recognize faces within an image, its memory usage is 150 MB.

### Benchmarks

Here are some original works that laid the fundamental groundwork for developing systematic benchmarks for inference machine learning systems.

**[MLPerf Inference Benchmark](https://github.com/mlcommons/inference):** MLPerf Inference is a comprehensive benchmark suite that assesses machine learning models' performance during the inference phase. It encompasses a variety of workloads, including image classification, object detection, and natural language processing, aiming to provide standardized and insightful metrics for evaluating different inference systems. It's metrics include:

MLPerf Inference is a comprehensive benchmark suite that assesses machine learning models' performance during the inference phase. It encompasses a variety of workloads, including image classification, object detection, and natural language processing, aiming to provide standardized and insightful metrics for evaluating different inference systems.

Metrics:

* Inference time
* Latency
* Throughput
* Accuracy
* Energy consumption

**[AI Benchmark](https://ai-benchmark.com/):** AI Benchmark is a benchmarking tool that evaluates the performance of AI and machine learning models on mobile devices and edge computing platforms. It includes tests for image classification, object detection, and natural language processing tasks, providing a detailed analysis of the inference performance on different hardware platforms. It's metrics include:

AI Benchmark is a benchmarking tool that evaluates the performance of AI and machine learning models on mobile devices and edge computing platforms. It includes tests for image classification, object detection, and natural language processing tasks, providing a detailed analysis of the inference performance on different hardware platforms.

Metrics:

* Inference time
* Latency
* Energy consumption
* Memory usage
* Throughput

**[OpenVINO toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html):** OpenVINO toolkit provides a benchmark tool to measure the performance of deep learning models for various tasks, such as image classification, object detection, and facial recognition, on Intel hardware. It offers detailed insights into the models' inference performance on different hardware configurations. It's metrics include:

Metrics:

* Inference time
* Throughput
* Latency
* CPU and GPU utilization

### Example Use Case

Suppose you were tasked with evaluating the inference performance of an object detection model on a specific edge device. Here's how you might approach structuring this benchmark:

1. **Define the Task**: In this case, the task is real-time object detection on video streams, identifying objects such as vehicles, pedestrians, and traffic signs.

2. **Select the Benchmark**: To align with your goal of evaluating inference on an edge device, the AI Benchmark is a suitable choice. It provides a standardized framework specifically for assessing inference performance on edge hardware, making it relevant to this scenario.

3. **Identify Key Metrics**: Now, determine the metrics that will help evaluate the model's inference performance. For this example, you might track:
   * **Inference Time**: How long does it take to process each video frame?
   * **Latency**: What is the delay in generating bounding boxes for detected objects?
   * **Energy Consumption**: How much power is used during inference?
   * **Throughput**: How many video frames are processed per second?

By measuring these metrics, you'll gain insights into how well the object detection model performs on the edge device. This can help identify any bottlenecks, such as slow frame processing or high energy consumption, and highlight areas for potential optimization to improve real-time performance.

:::{#exr-perf .callout-caution collapse="true" title="Inference Benchmarks - MLPerf"}

Get ready to put your AI models to the ultimate test! MLPerf is like the Olympics for machine learning performance. In this Colab, we'll use a toolkit called CK to run official MLPerf benchmarks, measure how fast and accurate your model is, and even use TVM to give it a super speed boost. Are you ready to see your model earn its medal?  
  
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/drive/1aywGlyD1ZRDtQTrQARVgL1882JcvmFK-?usp=sharing#scrollTo=tnyHAdErL72u)

:::

### Benchmark Task Selection

Selecting representative tasks for benchmarking machine learning systems is complex due to the varied applications, data types, and requirements across different domains. Machine learning is applied in fields such as healthcare, finance, natural language processing, and computer vision, each with unique tasks that may not be relevant or comparable to others. Key challenges in task selection include:

1. **Diversity of Applications and Data Types:** Tasks across domains involve different data types (e.g., text, images, video) and qualities, making it difficult to find benchmarks that universally represent ML challenges.
2. **Task Complexity and Resource Needs:** Tasks vary in complexity and resource demands, with some requiring substantial computational power and sophisticated models, while others can be addressed with simpler resources and methods.
3. **Privacy Concerns:** Tasks involving sensitive data, such as medical records or personal information, introduce ethical and privacy issues, making them unsuitable for general benchmarks.
4. **Evaluation Metrics:** Performance metrics vary widely across tasks, and results from one task often do not generalize to others, complicating comparisons and limiting insights from one benchmarked task to another.

Addressing these challenges is essential to designing meaningful benchmarks that are relevant across the diverse tasks encountered in machine learning, ensuring benchmarks provide useful, generalizable insights for both training and inference.

## Measuring Energy Efficiency

As machine learning capabilities expand, both in training and inference, concerns about increased power consumption and its ecological footprint have intensified. Addressing the sustainability of ML systems, a topic explored in more depth in the [Sustainable AI](../sustainable_ai/sustainable_ai.qmd) chapter, has thus become a key priority. This focus on sustainability has led to the development of standardized benchmarks designed to accurately measure energy efficiency. However, standardizing these methodologies poses challenges due to the need to accommodate vastly different scalesâfrom the microwatt consumption of TinyML devices to the megawatt demands of data center training systems. Moreover, ensuring that benchmarking is fair and reproducible requires accommodating the diverse range of hardware configurations and architectures in use today.

One example is the MLPerf Power benchmarking methodology [@tschand2024mlperf], which tackles these challenges by tailoring the methodologies for datacenter, edge inference, and tiny inference systems while measuring power consumption as comprehensively as possible for each scale. This methodology adapts to a variety of hardware, from general-purpose CPUs to specialized AI accelerators, while maintaining uniform measurement principles to ensure that comparisons are both fair and accurate across different platforms.

@fig-power-diagram illustrates the power measurement boundaries for different system scales, from TinyML devices to inference nodes and training racks. Each example highlights the components within the measurement boundary and those outside it. This setup allows for accurate reflection of the true energy costs associated with running ML workloads across various real-world scenarios, and ensures that the benchmark captures the full spectrum of energy consumption.

![MLPerf Power system measurement diagram. Source: @tschand2024mlperf.](images/png/power_component_diagram.png){#fig-power-diagram}

It is important to note that optimizing a system for performance may not lead to the most energy efficient execution. Oftentimes, sacrificing a small amount of performance or accuracy can lead to significant gains in energy efficiency, highlighting the importance of accurately benchmarking power metrics. Future insights from energy efficiency and sustainability benchmarking will enable us to optimize for more sustainable ML systems.

## Benchmark Example

To properly illustrate the components of a systems benchmark, we can look at the keyword spotting benchmark in MLPerf Tiny and explain the motivation behind each decision.

### Task

Keyword spotting was selected as a task because it is a common use case in TinyML that has been well-established for years. Additionally, the typical hardware used for keyword spotting differs substantially from the offerings of other benchmarks, such as MLPerf Inference's speech recognition task.

### Dataset

[Google Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) [@warden2018speech] was selected as the best dataset to represent the task. The dataset is well-established in the research community and has permissive licensing, allowing it to be easily used in a benchmark.

### Model

The next core component is the model, which will act as the primary workload for the benchmark. The model should be well established as a solution to the selected task rather than a state-of-the-art solution. The model selected is a simple depthwise separable convolution model. This architecture is not the state-of-the-art solution to the task, but it is well-established and not designed for a specific hardware platform like many state-of-the-art solutions. Despite being an inference benchmark, the benchmark also establishes a reference training recipe to be fully reproducible and transparent.

### Metrics

Latency was selected as the primary metric for the benchmark, as keyword spotting systems need to react quickly to maintain user satisfaction. Additionally, given that TinyML systems are often battery-powered, energy consumption is measured to ensure the hardware platform is efficient. The accuracy of the model is also measured to ensure that the optimizations applied by a submitter, such as quantization, don't degrade the accuracy beyond a threshold.

### Benchmark Harness

MLPerf Tiny uses [EEMBCs EnergyRunner benchmark harness](https://github.com/eembc/energyrunner) to load the inputs to the model and isolate and measure the device's energy consumption. When measuring energy consumption, it's critical to select a harness that is accurate at the expected power levels of the devices under test and simple enough not to become a burden for the benchmark participants.

### Baseline Submission

Baseline submissions are critical for contextualizing results and as a reference point to help participants get started. The baseline submission should prioritize simplicity and readability over state-of-the-art performance. The keyword spotting baseline uses a standard [STM microcontroller](https://www.st.com/en/microcontrollers-microprocessors.html) as its hardware and [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers) [@david2021tensorflow] as its inference framework.

### Challenges and Limitations

While benchmarking provides a structured methodology for performance evaluation in complex domains like artificial intelligence and computing, the process also poses several challenges. If not properly addressed, these challenges can undermine the credibility and accuracy of benchmarking results. Some of the predominant difficulties faced in benchmarking include the following:

* **Incomplete problem coverage:** Benchmark tasks may not fully represent the problem space. For instance, common image classification datasets like [CIFAR-10](https://www.cs.toronto.edu/kriz/cifar.html) have limited diversity in image types. Algorithms tuned for such benchmarks may fail to generalize well to real-world datasets.

* **Statistical insignificance:** Benchmarks must have enough trials and data samples to produce statistically significant results. For example, benchmarking an OCR model on only a few text scans may not adequately capture its true error rates.

* **Limited reproducibility:** Varying hardware, software versions, codebases, and other factors can reduce the reproducibility of benchmark results. MLPerf addresses this by providing reference implementations and environment specifications.

* **Misalignment with end goals:**  Benchmarks focusing only on speed or accuracy metrics may misalign real-world objectives like cost and power efficiency. Benchmarks must reflect all critical performance axes.

* **Rapid staleness:** Due to the rapid pace of advancements in AI and computing, benchmarks and their datasets can quickly become outdated. Maintaining up-to-date benchmarks is thus a persistent challenge.

But of all these, the most important challenge is benchmark engineering.

#### Hardware Lottery

The hardware lottery, first described by @hooker2021hardware, refers to the situation where a machine learning model's success or efficiency is significantly influenced by its compatibility with the underlying hardware [@chu2021discovering]. Some models perform exceptionally well not because they are intrinsically superior, but because they are optimized for specific hardware characteristics, such as the parallel processing capabilities of Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs).

For instance, @fig-hardware-lottery compares the performance of models across different hardware platforms. The multi-hardware models show comparable results to "MobileNetV3 Large min" on both the CPU uint8 and GPU configurations. However, these multi-hardware models demonstrate significant performance improvements over the MobileNetV3 Large baseline when run on the EdgeTPU and DSP hardware. This emphasizes the variable efficiency of multi-hardware models in specialized computing environments.

![Accuracy-latency trade-offs of multiple ML models and how they perform on various hardware. Source: @chu2021discovering](images/png/hardware_lottery.png){#fig-hardware-lottery}

Hardware lottery can introduce challenges and biases in benchmarking machine learning systems, as the model's performance is not solely dependent on the model's architecture or algorithm but also on the compatibility and synergies with the underlying hardware. This can make it difficult to compare different models fairly and to identify the best model based on its intrinsic merits. It can also lead to a situation where the community converges on models that are a good fit for the popular hardware of the day, potentially overlooking other models that might be superior but incompatible with the current hardware trends.

#### Benchmark Engineering

Hardware lottery occurs when a machine learning model unintentionally performs exceptionally well or poorly on a specific hardware setup due to unforeseen compatibility or incompatibility. The model is not explicitly designed or optimized for that particular hardware by the developers or engineers; rather, it happens to align or (mis)align with the hardware's capabilities or limitations. In this case, the model's performance on the hardware is a byproduct of coincidence rather than design.

In contrast to the accidental hardware lottery, benchmark engineering involves deliberately optimizing or designing a machine learning model to perform exceptionally well on specific hardware, often to win benchmarks or competitions. This intentional optimization might include tweaking the model's architecture, algorithms, or parameters to exploit the hardware's features and capabilities fully.

##### Problem

Benchmark engineering refers to tweaking or modifying an AI system to optimize performance on specific benchmark tests, often at the expense of generalizability or real-world performance. This can include adjusting hyperparameters, training data, or other aspects of the system specifically to achieve high scores on benchmark metrics without necessarily improving the overall functionality or utility of the system.

The motivation behind benchmark engineering often stems from the desire to achieve high-performance scores for marketing or competitive purposes. High benchmark scores can demonstrate the superiority of an AI system compared to competitors and can be a key selling point for potential users or investors. This pressure to perform well on benchmarks sometimes leads to prioritizing benchmark-specific optimizations over more holistic improvements to the system.

It can lead to several risks and challenges. One of the primary risks is that the AI system may perform better in real-world applications than the benchmark scores suggest. This can lead to user dissatisfaction, reputational damage, and potential safety or ethical concerns. Furthermore, benchmark engineering can contribute to a lack of transparency and accountability in the AI community, as it can be difficult to discern how much of an AI system's performance is due to genuine improvements versus benchmark-specific optimizations.

The AI community must prioritize transparency and accountability to mitigate the risks associated with benchmark engineering. This can include disclosing any optimizations or adjustments made specifically for benchmark tests and providing more comprehensive evaluations of AI systems that include real-world performance metrics and benchmark scores. Researchers and developers must prioritize holistic improvements to AI systems that improve their generalizability and functionality across various applications rather than focusing solely on benchmark-specific optimizations.

##### Issues

One of the primary problems with benchmark engineering is that it can compromise the real-world performance of AI systems. When developers focus on optimizing their systems to achieve high scores on specific benchmark tests, they may neglect other important system performance aspects crucial in real-world applications. For example, an AI system designed for image recognition might be engineered to perform exceptionally well on a benchmark test that includes a specific set of images but needs help to recognize images slightly different from those in the test set accurately.

Another area for improvement with benchmark engineering is that it can result in AI systems that lack generalizability. In other words, while the system may perform well on the benchmark test, it may need help handling a diverse range of inputs or scenarios. For instance, an AI model developed for natural language processing might be engineered to achieve high scores on a benchmark test that includes a specific type of text but fails to process text that falls outside of that specific type accurately.

It can also lead to misleading results. When AI systems are engineered to perform well on benchmark tests, the results may not accurately reflect the system's true capabilities. This can be problematic for users or investors who rely on benchmark scores to make informed decisions about which AI systems to use or invest in. For example, an AI system engineered to achieve high scores on a benchmark test for speech recognition might need to be more capable of accurately recognizing speech in real-world situations, leading users or investors to make decisions based on inaccurate information.

##### Mitigation

There are several ways to mitigate benchmark engineering. Transparency in the benchmarking process is crucial to maintaining benchmark accuracy and reliability. This involves clearly disclosing the methodologies, data sets, and evaluation criteria used in benchmark tests, as well as any optimizations or adjustments made to the AI system for the purpose of the benchmark.

One way to achieve transparency is through the use of open-source benchmarks. Open-source benchmarks are made publicly available, allowing researchers, developers, and other stakeholders to review, critique, and contribute to them, thereby ensuring their accuracy and reliability. This collaborative approach also facilitates sharing best practices and developing more robust and comprehensive benchmarks.

The modular design of MLPerf Tiny connects to the problem of benchmark engineering by providing a structured yet flexible approach that encourages a balanced evaluation of TinyML. In benchmark engineering, systems may be overly optimized for specific benchmarks, leading to inflated performance scores that don't necessarily translate to real-world effectiveness. MLPerf Tiny's modular design aims to address this issue by allowing contributors to swap out and test specific components within a standardized framework, such as hardware, quantization techniques, or inference models. The reference implementations, highlighted in green and orange in @fig-ml-perf, provide a baseline for results, enabling flexible yet controlled testing by specifying which components can be modified. This structure supports transparency and flexibility, enabling a focus on genuine improvements rather than benchmark-specific optimizations.

![Modular design of the MLPerf Tiny benchmark, showing the reference implementation with modifiable components. This modular approach enables flexible, targeted testing while maintaining a standardized baseline. Source: @banbury2021mlperf.](images/png/mlperf_tiny.png){#fig-ml-perf}

Another method for achieving transparency is through peer review of benchmarks. This involves having independent experts review and validate the benchmark's methodology, data sets, and results to ensure their credibility and reliability. Peer review can provide a valuable means of verifying the accuracy of benchmark tests and help build confidence in the results.

Standardization of benchmarks is another important solution to mitigate benchmark engineering. Standardized benchmarks provide a common framework for evaluating AI systems, ensuring consistency and comparability across different systems and applications. This can be achieved by developing industry-wide standards and best practices for benchmarking and through common metrics and evaluation criteria.

Third-party verification of results can also be valuable in mitigating benchmark engineering. This involves having an independent third party verify the results of a benchmark test to ensure their credibility and reliability. Third-party verification can build confidence in the results and provide a valuable means of validating the performance and capabilities of AI systems.

## Model Benchmarking

Benchmarking machine learning models is important for determining the effectiveness and efficiency of various machine learning algorithms in solving specific tasks or problems. By analyzing the results obtained from benchmarking, developers and researchers can identify their models' strengths and weaknesses, leading to more informed decisions on model selection and further optimization.

The evolution and progress of machine learning models are intrinsically linked to the availability and quality of data sets. In machine learning, data acts as the raw material that powers the algorithms, allowing them to learn, adapt, and ultimately perform tasks that were traditionally the domain of humans. Therefore, it is important to understand this history.

### Historical Context

Machine learning datasets have a rich history and have evolved significantly over the years, growing in size, complexity, and diversity to meet the ever-increasing demands of the field. Let's take a closer look at this evolution, starting from one of the earliest and most iconic datasets -- MNIST.

#### MNIST (1998)

The [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist), created by Yann LeCun, Corinna Cortes, and Christopher J.C. Burges in 1998, can be considered a cornerstone in the history of machine learning datasets. It comprises 70,000 labeled 28x28 pixel grayscale images of handwritten digits (0-9). MNIST has been widely used for benchmarking algorithms in image processing and machine learning as a starting point for many researchers and practitioners. @fig-mnist shows some examples of handwritten digits.

![MNIST handwritten digits. Source: [Suvanjanprasai](https://en.wikipedia.org/wiki/File:MnistExamplesModified.png)](images/png/mnist.png){#fig-mnist}

#### ImageNet (2009)

Fast forward to 2009, and we see the introduction of the [ImageNet dataset](https://www.tensorflow.org/datasets/catalog/imagenet2012), which marked a significant leap in the scale and complexity of datasets. ImageNet consists of over 14 million labeled images spanning more than 20,000 categories. Fei-Fei Li and her team developed it to advance object recognition and computer vision research. The dataset became synonymous with the ImageNet [Large Scale Visual Recognition Challenge (LSVRC)](https://www.image-net.org/challenges/LSVRC/), an annual competition crucial in developing deep learning models, including the famous AlexNet in 2012.

#### COCO (2014)

The [Common Objects in Context (COCO) dataset](https://cocodataset.org/) [@lin2014microsoft], released in 2014, further expanded the landscape of machine learning datasets by introducing a richer set of annotations. COCO consists of images containing complex scenes with multiple objects, and each image is annotated with object bounding boxes, segmentation masks, and captions, as shown in @fig-coco. This dataset has been instrumental in advancing research in object detection, segmentation, and image captioning.

![Example images from the COCO dataset. Source: [Coco](https://cocodataset.org/)](images/png/coco.png){#fig-coco}

#### GPT-3 (2020)

While the above examples primarily focus on image datasets, there have also been significant developments in text datasets. One notable example is GPT-3 [@brown2020language], developed by OpenAI. GPT-3 is a language model trained on diverse internet text. Although the dataset used to train GPT-3 is not publicly available, the model itself, consisting of 175 billion parameters, is a testament to the scale and complexity of modern machine learning datasets and models.

#### Present and Future

Today, we have a plethora of datasets spanning various domains, including healthcare, finance, social sciences, and more. The following characteristics help us taxonomize the space and growth of machine learning datasets that fuel model development.

1. **Diversity of Data Sets:** The variety of data sets available to researchers and engineers has expanded dramatically, covering many fields, including natural language processing, image recognition, and more. This diversity has fueled the development of specialized machine-learning models tailored to specific tasks, such as translation, speech recognition, and facial recognition.

2. **Volume of Data:** The sheer volume of data that has become available in the digital age has also played a crucial role in advancing machine learning models. Large data sets enable models to capture the complexity and nuances of real-world phenomena, leading to more accurate and reliable predictions.

3. **Quality and Cleanliness of Data:** The quality of data is another critical factor that influences the performance of machine learning models. Clean, well-labeled, and unbiased data sets are essential for training models that are robust and fair.

4. **Open Access to Data:** The availability of open-access data sets has also contributed significantly to machine learning's progress. Open data allows researchers from around the world to collaborate, share insights, and build upon each other's work, leading to faster innovation and the development of more advanced models.

5. **Ethics and Privacy Concerns:** As data sets grow in size and complexity, ethical considerations and privacy concerns become increasingly important. There is an ongoing debate about the balance between leveraging data for machine learning advancements and protecting individuals' privacy rights.

The development of machine learning models relies heavily on the availability of diverse, large, high-quality, and open-access data sets. As we move forward, addressing the ethical considerations and privacy concerns associated with using large data sets is crucial to ensure that machine learning technologies benefit society. There is a growing awareness that data acts as the rocket fuel for machine learning, driving and fueling the development of machine learning models. Consequently, more focus is being placed on developing the data sets themselves. We will explore this in further detail in the data benchmarking section.

### Model Metrics

Machine learning model evaluation has evolved from a narrow focus on accuracy to a more comprehensive approach considering a range of factors, from ethical considerations and real-world applicability to practical constraints like model size and efficiency. This shift reflects the field's maturation as machine learning models are increasingly applied in diverse, complex real-world scenarios.

#### Accuracy

Accuracy is one of the most intuitive and commonly used metrics for evaluating machine learning models. In the early stages of machine learning, accuracy was often the primary, if not the only, metric considered when evaluating model performance. However, as the field has evolved, it's become clear that relying solely on accuracy can be misleading, especially in applications where certain types of errors carry significant consequences.

Consider the example of a medical diagnosis model with an accuracy of 95%. While at first glance this may seem impressive, we must look deeper to assess the model's performance fully. Suppose the model fails to accurately diagnose severe conditions that, while rare, can have severe consequences; its high accuracy may not be as meaningful. A well-known example of this limitation is [Google's diabetic retinopathy model](https://about.google/intl/ALL_us/stories/seeingpotential/). While it achieved high accuracy in lab settings, it encountered challenges when deployed in real-world clinics in Thailand, where variations in patient populations, image quality, and environmental factors reduced its effectiveness. This example illustrates that even models with high accuracy need to be tested for their ability to generalize across diverse, unpredictable conditions to ensure reliability and impact in real-world settings.

Similarly, if the model performs well on average but exhibits significant disparities in performance across different demographic groups, this, too, would be cause for concern. The evolution of machine learning has thus seen a shift towards a more holistic approach to model evaluation, taking into account not just accuracy, but also other crucial factors such as fairness, transparency, and real-world applicability. A prime example is the [Gender Shades project](http://gendershades.org/) at MIT Media Lab, led by Joy Buolamwini, highlighting biases by performing better on lighter-skinned and male faces compared to darker-skinned and female faces.

While accuracy remains essential for evaluating machine learning models, a comprehensive approach is needed to fully assess performance. This includes additional metrics for fairness, transparency, and real-world applicability, along with rigorous testing across diverse datasets to identify and address biases. This holistic evaluation approach reflects the field's growing awareness of real-world implications in deploying models.

#### Fairness

Fairness in machine learning involves ensuring that models perform consistently across diverse groups, especially in high-impact applications like loan approvals, hiring, and criminal justice. Relying solely on accuracy can be misleading if the model exhibits biased outcomes across demographic groups. For example, a loan approval model with high accuracy may still consistently deny loans to certain groups, raising questions about its fairness.

Bias in models can arise directly, when sensitive attributes like race or gender influence decisions, or indirectly, when neutral features correlate with these attributes, affecting outcomes. Simply relying on accuracy can be insufficient when evaluating models. For instance, consider a loan approval model with a 95% accuracy rate. While this figure may appear impressive at first glance, it does not reveal how the model performs across different demographic groups. For instance, a well-known example is the COMPAS tool used in the US criminal justice system, which showed racial biases in predicting recidivism despite not explicitly using race as a variable.

Addressing fairness requires analyzing a model's performance across groups, identifying biases, and applying corrective measures like re-balancing datasets or using fairness-aware algorithms. Researchers and practitioners continuously develop metrics and methodologies tailored to specific use cases to evaluate fairness in real-world scenarios. For example, disparate impact analysis, demographic parity, and equal opportunity are some of the metrics employed to assess fairness. Additionally, transparency and interpretability of models are fundamental to achieving fairness. Tools like [AI Fairness 360](https://ai-fairness-360.org/) and [Fairness Indicators](https://www.tensorflow.org/tfx/guide/fairness_indicators) help explain how a model makes decisions, allowing developers to detect and correct fairness issues in machine learning models.

While accuracy is a valuable metric, it doesn't always provide the full picture; assessing fairness ensures models are effective across real-world scenarios. Ensuring fairness in machine learning models, particularly in applications that significantly impact people's lives, requires rigorous evaluation of the model's performance across diverse groups, careful identification and mitigation of biases, and implementation of transparency and interpretability measures.

#### Complexity

##### Parameters

In the initial stages of machine learning, model benchmarking often relied on parameter counts as a proxy for model complexity. The rationale was that more parameters typically lead to a more complex model, which should, in turn, deliver better performance. However, this approach overlooks the practical costs associated with processing large models. As parameter counts increase, so do the computational resources required, making such models impractical for deployment in real-world scenarios, particularly on devices with limited processing power.

Relying on parameter counts as a proxy for model complexity also fails to consider the model's efficiency. A well-optimized model with fewer parameters can often achieve comparable or even superior performance to a larger model. For instance, MobileNets, developed by Google, is a family of models designed specifically for mobile and edge devices. They used depth-wise separable convolutions to reduce parameter counts and computational demands while still maintaining strong performance.

In light of these limitations, the field has moved towards a more holistic approach to model benchmarking that considers parameter counts and other crucial factors such as floating-point operations per second (FLOPs), memory consumption, and latency. This comprehensive approach balances performance with deployability, ensuring that models are not only accurate but also efficient and suitable for real-world applications.

##### FLOPS

FLOPs, or floating-point operations per second, have become a critical metric for representing a model's computational load. Traditionally, parameter count was used as a proxy for model complexity, based on the assumption that more parameters would yield better performance. However, this approach overlooks the computational cost of processing these parameters, which can impact a model's usability in real-world scenarios with limited resources.

FLOPs measure the number of floating-point operations a model performs to generate a prediction. A model with many FLOPs requires substantial computational resources to process the vast number of operations, which may render it impractical for certain applications. Conversely, a model with a lower FLOP count is more lightweight and can be easily deployed in scenarios where computational resources are limited. @fig-flops, from [@bianco2018benchmark], illustrates the trade-off between ImageNet accuracy, FLOPs, and parameter count, showing that some architectures achieve higher efficiency than others.

![A graph that depicts the top-1 imagenet accuracy vs. the FLOP count of a model along with the model's parameter count. The figure shows a overall tradeoff between model complexity and accuracy, although some model architectures are more efficiency than others. Source: @bianco2018benchmark.](images/png/model_FLOPS_VS_TOP_1.png){#fig-flops}

Let's consider an example. BERT---Bidirectional Encoder Representations from Transformers [@devlin2018bert]---is a popular natural language processing model, has over 340 million parameters, making it a large model with high accuracy and impressive performance across various tasks. However, the sheer size of BERT, coupled with its high FLOP count, makes it a computationally intensive model that may not be suitable for real-time applications or deployment on edge devices with limited computational capabilities. In light of this, there has been a growing interest in developing smaller models that can achieve similar performance levels as their larger counterparts while being more efficient in computational load. DistilBERT, for instance, is a smaller version of BERT that retains 97% of its performance while being 40% smaller in terms of parameter count. The size reduction also translates to a lower FLOP count, making DistilBERT a more practical choice for resource-constrained scenarios.

While parameter count indicates model size, it does not fully capture the computational cost. FLOPs provide a more accurate measure of computational load, highlighting the practical trade-offs in model deployment. This shift from parameter count to FLOPs reflects the field's growing awareness of deployment challenges in diverse settings.

##### Efficiency

Efficiency metrics, such as memory consumption and latency/throughput, have also gained prominence. These metrics are particularly crucial when deploying models on edge devices or in real-time applications, as they measure how quickly a model can process data and how much memory it requires. In this context, Pareto curves are often used to visualize the trade-off between different metrics, helping stakeholders decide which model best suits their needs.

### Lessons Learned

Model benchmarking has offered us several valuable insights that can be leveraged to drive innovation in system benchmarks. The progression of machine learning models has been profoundly influenced by the advent of leaderboards and the open-source availability of models and datasets. These elements have served as significant catalysts, propelling innovation and accelerating the integration of cutting-edge models into production environments. However, as we will explore further, these are not the only contributors to the development of machine learning benchmarks.

Leaderboards play a vital role in providing an objective and transparent method for researchers and practitioners to evaluate the efficacy of different models, ranking them based on their performance in benchmarks. This system fosters a competitive environment, encouraging the development of models that are not only accurate but also efficient. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a prime example of this, with its annual leaderboard significantly contributing to developing groundbreaking models such as AlexNet.

Open-source access to state-of-the-art models and datasets further democratizes machine learning, facilitating collaboration among researchers and practitioners worldwide. This open access accelerates the process of testing, validation, and deployment of new models in production environments, as evidenced by the widespread adoption of models like BERT and GPT-3 in various applications, from natural language processing to more complex, multi-modal tasks.

Community collaboration platforms like Kaggle have revolutionized the field by hosting competitions that unite data scientists from across the globe to solve intricate problems. Specific benchmarks serve as the goalposts for innovation and model development.

Moreover, the availability of diverse and high-quality datasets is paramount in training and testing machine learning models. Datasets such as ImageNet have played an instrumental role in the evolution of image recognition models, while extensive text datasets have facilitated advancements in natural language processing models.

Lastly, the contributions of academic and research institutions must be supported. Their role in publishing research papers, sharing findings at conferences, and fostering collaboration between various institutions has significantly contributed to advancing machine learning models and benchmarks.

#### Emerging Trends

As machine learning models become more sophisticated, so do the benchmarks required to assess them accurately. There are several emerging benchmarks and datasets that are gaining popularity due to their ability to evaluate models in more complex and realistic scenarios:

**Multimodal Datasets:** These datasets contain multiple data types, such as text, images, and audio, to represent real-world situations better. An example is the VQA (Visual Question Answering) dataset [@antol2015vqa], where models' ability to answer text-based questions about images is tested.

**Fairness and Bias Evaluation:** There is an increasing focus on creating benchmarks assessing machine learning models' fairness and bias. Examples include the [AI Fairness 360](https://ai-fairness-360.org/) toolkit, which offers a comprehensive set of metrics and datasets for evaluating bias in models.

**Out-of-Distribution Generalization:** Testing how well models perform on data different from the original training distribution. This evaluates the model's ability to generalize to new, unseen data. Example benchmarks are Wilds [@koh2021wilds], RxRx, and ANC-Bench.

**Adversarial Robustness:** Evaluating model performance under adversarial attacks or perturbations to the input data. This tests the model's robustness. Example benchmarks are ImageNet-A [@hendrycks2021natural], ImageNet-C [@xie2020adversarial], and CIFAR-10.1.

**Real-World Performance:** Testing models on real-world datasets that closely match end tasks rather than just canned benchmark datasets. Examples are medical imaging datasets for healthcare tasks or customer support chat logs for dialogue systems.

**Energy and Compute Efficiency:** Benchmarks that measure the computational resources required to achieve a particular accuracy. This evaluates the model's Efficiency. Examples are MLPerf and Greenbench, already discussed in the Systems benchmarking section.

**Interpretability and Explainability:** Benchmarks that assess how easy it is to understand and explain a model's internal logic and predictions. Example metrics are faithfulness to input gradients and coherence of explanations.

### Limitations and Challenges

While model benchmarks are an essential tool in assessing machine learning models, several limitations and challenges should be addressed to ensure that they accurately reflect a model's performance in real-world scenarios.

**Dataset does not Correspond to Real-World Scenarios:** Often, the data used in model benchmarks is cleaned and preprocessed to such an extent that it may need to accurately represent the data that a model would encounter in real-world applications. This idealized data version can lead to overestimating a model's performance. In the case of the ImageNet dataset, the images are well-labeled and categorized. Still, in a real-world scenario, a model may need to deal with blurry images that could be better lit or taken from awkward angles. This discrepancy can significantly affect the model's performance.

**Sim2Real Gap:** The Sim2Real gap refers to the difference in the performance of a model when transitioning from a simulated environment to a real-world environment. This gap is often observed in robotics, where a robot trained in a simulated environment struggles to perform tasks in the real world due to the complexity and unpredictability of real-world environments. A robot trained to pick up objects in a simulated environment may need help to perform the same task in the real world because the simulated environment does not accurately represent the complexities of real-world physics, lighting, and object variability.

**Challenges in Creating Datasets:** Creating a dataset for model benchmarking is a challenging task that requires careful consideration of various factors such as data quality, diversity, and representation. As discussed in the data engineering section, ensuring that the data is clean, unbiased, and representative of the real-world scenario is crucial for the accuracy and reliability of the benchmark. For example, when creating a dataset for a healthcare-related task, it is important to ensure that the data is representative of the entire population and not biased towards a particular demographic. This ensures that the model performs well across diverse patient populations.

Model benchmarks are essential in measuring the capability of a model architecture in solving a fixed task, but it is important to address the limitations and challenges associated with them. This includes ensuring that the dataset accurately represents real-world scenarios, addressing the Sim2Real gap, and overcoming the challenges of creating unbiased and representative datasets. By addressing these challenges and many others, we can ensure that model benchmarks provide a more accurate and reliable assessment of a model's performance in real-world applications.

The [Speech Commands dataset](https://arxiv.org/pdf/1804.03209.pdf) and its successor [MSWC](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/fe131d7f5a6b38b23cc967316c13dae2-Paper-round2.pdf), are common benchmarks for one of the quintessential TinyML applications, keyword spotting. Speech commands establish streaming error metrics beyond the standard top-1 classification accuracy more relevant to the keyword spotting use case. Using case-relevant metrics is what elevates a dataset to a model benchmark.

## Data Benchmarking

For the past several years, AI has focused on developing increasingly sophisticated machine learning models like large language models. The goal has been to create models capable of human-level or superhuman performance on a wide range of tasks by training them on massive datasets. This model-centric approach produced rapid progress, with models attaining state-of-the-art results on many established benchmarks. @fig-superhuman-perf shows the performance of AI systems relative to human performance (marked by the horizontal line at 0) across five applications: handwriting recognition, speech recognition, image recognition, reading comprehension, and language understanding. Over the past decade, the AI performance has surpassed that of humans.

![AI vs human performane. Source: @kiela2021dynabench.](images/png/dynabench.png){#fig-superhuman-perf}

However, growing concerns about issues like bias, safety, and robustness persist even in models that achieve high accuracy on standard benchmarks. Additionally, some popular datasets used for evaluating models are beginning to saturate, with models reaching near-perfect performance on existing test splits [@kiela2021dynabench]. As a simple example, there are test images in the classic MNIST handwritten digit dataset that may look indecipherable to most human evaluators but were assigned a label when the dataset was created - models that happen to agree with those labels may appear to exhibit superhuman performance but instead may only be capturing idiosyncrasies of the labeling and acquisition process from the dataset's creation in 1994. In the same spirit, computer vision researchers now ask, "Are we done with ImageNet?" [@beyer2020we]. This highlights limitations in the conventional model-centric approach of optimizing accuracy on fixed datasets through architectural innovations.

An alternative paradigm is emerging called data-centric AI. Rather than treating data as static and focusing narrowly on model performance, this approach recognizes that models are only as good as their training data. So, the emphasis shifts to curating high-quality datasets that better reflect real-world complexity, developing more informative evaluation benchmarks, and carefully considering how data is sampled, preprocessed, and augmented. The goal is to optimize model behavior by improving the data rather than just optimizing metrics on flawed datasets. Data-centric AI critically examines and enhances the data itself to produce beneficial AI. This reflects an important evolution in mindset as the field addresses the shortcomings of narrow benchmarking.

This section will explore the key differences between model-centric and data-centric approaches to AI. This distinction has important implications for how we benchmark AI systems. Specifically, we will see how focusing on data quality and Efficiency can directly improve machine learning performance as an alternative to optimizing model architectures solely. The data-centric approach recognizes that models are only as good as their training data. So, enhancing data curation, evaluation benchmarks, and data handling processes can produce AI systems that are safer, fairer, and more robust. Rethinking benchmarking to prioritize data alongside models represents an important evolution as the field strives to deliver trustworthy real-world impact.

### Limitations of Model-Centric AI

In the model-centric AI era, a prominent characteristic was the development of complex model architectures. Researchers and practitioners dedicated substantial effort to devising sophisticated and intricate models in the quest for superior performance. This frequently involved the incorporation of additional layers and the fine-tuning of a multitude of hyperparameters to achieve incremental improvements in accuracy. Concurrently, there was a significant emphasis on leveraging advanced algorithms. These algorithms, often at the forefront of the latest research, were employed to improve the performance of AI models. The primary aim of these algorithms was to optimize the learning process of models, thereby extracting maximal information from the training data.

While the model-centric approach has been central to many advancements in AI, it has several areas for improvement. First, the development of complex model architectures can often lead to overfitting. This is when the model performs well on the training data but needs to generalize to new, unseen data. The additional layers and complexity can capture noise in the training data as if it were a real pattern, harming the model's performance on new data.

Second, relying on advanced algorithms can sometimes obscure the real understanding of a model's functioning. These algorithms often act as a black box, making it difficult to interpret how the model is making decisions. This lack of transparency can be a significant hurdle, especially in critical applications such as healthcare and finance, where understanding the model's decision-making process is crucial.

Third, the emphasis on achieving state-of-the-art results on benchmark datasets can sometimes be misleading. These datasets need to represent the complexities and variability of real-world data more fully. A model that performs well on a benchmark dataset may not necessarily generalize well to new, unseen data in a real-world application. This discrepancy can lead to false confidence in the model's capabilities and hinder its practical applicability.

Lastly, the model-centric approach often relies on large labeled datasets for training. However, obtaining such datasets takes time and effort in many real-world scenarios. This reliance on large datasets also limits AI's applicability in domains where data is scarce or expensive to label.

As a result of the above reasons, and many more, the AI community is shifting to a more data-centric approach. Rather than focusing just on model architecture, researchers are now prioritizing curating high-quality datasets, developing better evaluation benchmarks, and considering how data is sampled and preprocessed. The key idea is that models are only as good as their training data. So, focusing on getting the right data will allow us to develop AI systems that are more fair, safe, and aligned with human values. This data-centric shift represents an important change in mindset as AI progresses.

### The Shift Toward Data-centric AI

Data-centric AI is a paradigm that emphasizes the importance of high-quality, well-labeled, and diverse datasets in developing AI models. In contrast to the model-centric approach, which focuses on refining and iterating on the model architecture and algorithm to improve performance, data-centric AI prioritizes the quality of the input data as the primary driver of improved model performance. High-quality data is [clean, well-labeled](https://landing.ai/blog/tips-for-a-data-centric-ai-approach/) and representative of the real-world scenarios the model will encounter. In contrast, low-quality data can lead to poor model performance, regardless of the complexity or sophistication of the model architecture.

Data-centric AI puts a strong emphasis on the cleaning and labeling of data. Cleaning involves the removal of outliers, handling missing values, and addressing other data inconsistencies. Labeling, on the other hand, involves assigning meaningful and accurate labels to the data. Both these processes are crucial in ensuring that the AI model is trained on accurate and relevant data. Another important aspect of the data-centric approach is data augmentation. This involves artificially increasing the size and diversity of the dataset by applying various transformations to the data, such as rotation, scaling, and flipping training images. Data augmentation helps in improving the model's robustness and generalization capabilities.

There are several benefits to adopting a data-centric approach to AI development. First and foremost, it leads to improved model performance and generalization capabilities. By ensuring that the model is trained on high-quality, diverse data, the model can better generalize to new, unseen data [@gaviria2022dollar].

Additionally, a data-centric approach can often lead to simpler models that are easier to interpret and maintain. This is because the emphasis is on the data rather than the model architecture, meaning simpler models can achieve high performance when trained on high-quality data.

The shift towards data-centric AI represents a significant paradigm shift. By prioritizing the quality of the input data, this approach tries to model performance and generalization capabilities, ultimately leading to more robust and reliable AI systems. @fig-data-vs-model illustrates this difference. As we continue to advance in our understanding and application of AI, the data-centric approach is likely to play an important role in shaping the future of this field.

![Model-centric vs. Data-centric ML development. Source: [NVIDIA](https://blogs.nvidia.com/blog/difference-deep-learning-training-inference-ai/)](images/png/datavsmodelai.png){#fig-data-vs-model}

### Benchmarking Data

Data benchmarking focuses on evaluating common issues in datasets, such as identifying label errors, noisy features, representation imbalance (for example, out of the 1000 classes in Imagenet-1K, there are over 100 categories which are just types of dogs), class imbalance (where some classes have many more samples than others), whether models trained on a given dataset can generalize to out-of-distribution features, or what types of biases might exist in a given dataset [@gaviria2022dollar]. In its simplest form, data benchmarking seeks to improve accuracy on a test set by removing noisy or mislabeled training samples while keeping the model architecture fixed. Recent competitions in data benchmarking have invited participants to submit novel augmentation strategies and active learning techniques.

Data-centric techniques continue to gain attention in benchmarking, especially as foundation models are increasingly trained on self-supervised objectives. Compared to smaller datasets like Imagenet-1K, massive datasets commonly used in self-supervised learning, such as Common Crawl, OpenImages, and LAION-5B, contain higher amounts of noise, duplicates, bias, and potentially offensive data.

[DataComp](https://www.datacomp.ai/) is a recently launched dataset competition that targets the evaluation of large corpora. DataComp focuses on language-image pairs used to train CLIP models. The introductory whitepaper finds that when the total compute budget for training is constant, the best-performing CLIP models on downstream tasks, such as ImageNet classification, are trained on just 30% of the available training sample pool. This suggests that proper filtering of large corpora is critical to improving the accuracy of foundation models. Similarly, Demystifying CLIP Data [@xu2023demystifying] asks whether the success of CLIP is attributable to the architecture or the dataset.

[DataPerf](https://www.dataperf.org/) is another recent effort focusing on benchmarking data in various modalities. DataPerf provides rounds of online competition to spur improvement in datasets. The inaugural offering launched with challenges in vision, speech, acquisition, debugging, and text prompting for image generation.

### Data Efficiency

As machine learning models grow larger and more complex and compute resources become more scarce in the face of rising demand, it becomes challenging to meet the computation requirements even with the largest machine learning fleets. To overcome these challenges and ensure machine learning system scalability, it is necessary to explore novel opportunities that increase conventional approaches to resource scaling.

Improving data quality can be a useful method to impact machine learning system performance significantly. One of the primary benefits of enhancing data quality is the potential to reduce the size of the training dataset while still maintaining or even improving model performance. This data size reduction directly relates to the amount of training time required, thereby allowing models to converge more quickly and efficiently. Achieving this balance between data quality and dataset size is a challenging task that requires the development of sophisticated methods, algorithms, and techniques.

Several approaches can be taken to improve data quality. These methods include and are not limited to the following:

* **Data Cleaning:** This involves handling missing values, correcting errors, and removing outliers. Clean data ensures that the model is not learning from noise or inaccuracies.
* **Data Interpretability and Explainability:** Common techniques include LIME [@ribeiro2016should], which provides insight into the decision boundaries of classifiers, and Shapley values [@lundberg2017unified], which estimate the importance of individual samples in contributing to a model's predictions.
* **Feature Engineering:** Transforming or creating new features can significantly improve model performance by providing more relevant information for learning.
* **Data Augmentation:** Augmenting data by creating new samples through various transformations can help improve model robustness and generalization.
* **Active Learning:** This is a semi-supervised learning approach where the model actively queries a human oracle to label the most informative samples [@coleman2022similarity]. This ensures that the model is trained on the most relevant data.
* **Dimensionality Reduction:** Techniques like PCA can reduce the number of features in a dataset, thereby reducing complexity and training time.

There are many other methods in the wild. But the goal is the same. Refining the dataset and ensuring it is of the highest quality can reduce the training time required for models to converge. However, achieving this requires developing and implementing sophisticated methods, algorithms, and techniques that can clean, preprocess, and augment data while retaining the most informative samples. This is an ongoing challenge that will require continued research and innovation in the field of machine learning.

## The Trifecta

While system, model, and data benchmarks have traditionally been studied in isolation, there is a growing recognition that to understand and advance AI fully, we must take a more holistic view. By iterating between benchmarking systems, models, and datasets together, novel insights that are not apparent when these components are analyzed separately may emerge. System performance impacts model accuracy, model capabilities drive data needs, and data characteristics shape system requirements.

Benchmarking the triad of system, model, and data in an integrated fashion will likely lead to discoveries about the co-design of AI systems, the generalization properties of models, and the role of data curation and quality in enabling performance. Rather than narrow benchmarks of individual components, the future of AI requires benchmarks that evaluate the symbiotic relationship between computing platforms, algorithms, and training data. This systems-level perspective will be critical to overcoming current limitations and unlocking the next level of AI capabilities.

@fig-benchmarking-trifecta illustrates the many potential ways to interplay data benchmarking, model benchmarking, and system infrastructure benchmarking together. Exploring these intricate interactions is likely to uncover new optimization opportunities and enhancement capabilities. The data, model, and system benchmark triad offers a rich space for co-design and co-optimization.

![Benchmarking trifecta.](images/png/benchmarking_trifecta.png){#fig-benchmarking-trifecta}

While this integrated perspective represents an emerging trend, the field has much more to discover about the synergies and trade-offs between these components. As we iteratively benchmark combinations of data, models, and systems, new insights that remain hidden when these elements are studied in isolation will emerge. This multifaceted benchmarking approach charting the intersections of data, algorithms, and hardware promises to be a fruitful avenue for major progress in AI, even though it is still in its early stages.

## Benchmarks for Emerging Technologies

Given their significant differences from existing techniques, emerging technologies can be particularly challenging to design benchmarks for. Standard benchmarks used for existing technologies may not highlight the key features of the new approach. In contrast, new benchmarks may be seen as contrived to favor the emerging technology over others. They may be so different from existing benchmarks that they cannot be understood and lose insightful value. Thus, benchmarks for emerging technologies must balance fairness, applicability, and ease of comparison with existing benchmarks.

An example of emerging technology where benchmarking has proven to be especially difficult is in [Neuromorphic Computing](@sec-neuromorphic). Using the brain as a source of inspiration for scalable, robust, and energy-efficient general intelligence, neuromorphic computing [@schuman2022opportunities] directly incorporates biologically realistic mechanisms in both computing algorithms and hardware, such as spiking neural networks [@maass1997networks] and non-von Neumann architectures for executing them [@davies2018loihi; @modha2023neural]. From a full-stack perspective of models, training techniques, and hardware systems, neuromorphic computing differs from conventional hardware and AI. Thus, there is a key challenge in developing fair and useful benchmarks for guiding the technology.

An ongoing initiative to develop standard neuromorphic benchmarks is NeuroBench [@yik2023neurobench]. To suitably benchmark neuromorphic, NeuroBench follows high-level principles of _inclusiveness_ through task and metric applicability to both neuromorphic and non-neuromorphic solutions, _actionability_ of implementation using common tooling, and _iterative_ updates to continue to ensure relevance as the field rapidly grows. NeuroBench and other benchmarks for emerging technologies provide critical guidance for future techniques, which may be necessary as the scaling limits of existing approaches draw nearer.

## Conclusion

What gets measured gets improved. This chapter has explored the multifaceted nature of benchmarking spanning systems, models, and data. Benchmarking is important to advancing AI by providing the essential measurements to track progress.

ML system benchmarks enable optimization across speed, Efficiency, and scalability metrics. Model benchmarks drive innovation through standardized tasks and metrics beyond accuracy. Data benchmarks highlight issues of quality, balance, and representation.

Importantly, evaluating these components in isolation has limitations. In the future, more integrated benchmarking will likely be used to explore the interplay between system, model, and data benchmarks. This view promises new insights into co-designing data, algorithms, and infrastructure.

As AI grows more complex, comprehensive benchmarking becomes even more critical. Standards must continuously evolve to measure new capabilities and reveal limitations. Close collaboration between industry, academics, national labels, etc., is essential to developing benchmarks that are rigorous, transparent, and socially beneficial.

Benchmarking provides the compass to guide progress in AI. By persistently measuring and openly sharing results, we can navigate toward performant, robust, and trustworthy systems. If AI is to serve societal and human needs properly, it must be benchmarked with humanity's best interests in mind. To this end, there are emerging areas, such as benchmarking the safety of AI systems, but that's for another day and something we can discuss further in Generative AI!

Benchmarking is a continuously evolving topic. The article [The Olympics of AI: Benchmarking Machine Learning Systems](https://towardsdatascience.com/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b) covers several emerging subfields in AI benchmarking, including robotics, extended reality, and neuromorphic computing that we encourage the reader to pursue.

## Resources {#sec-benchmarking-ai-resource}

Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.

:::{.callout-note collapse="false"}

#### Slides

These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.

* [Why is benchmarking important?](https://docs.google.com/presentation/d/17udz3gxeYF3r3X1r4ePwu1I9H8ljb53W3ktFSmuDlGs/edit?usp=drive_link&resourcekey=0-Espn0a0x81kl2txL_jIWjw)

* [Embedded inference benchmarking.](https://docs.google.com/presentation/d/18PI_0xmcW1xwwfcjmj25PikqBM_92vQfOXFV4hah-6I/edit?resourcekey=0-KO3HQcDAsR--jgbKd5cp4w#slide=id.g94db9f9f78_0_2)

:::

:::{.callout-important collapse="false"}

#### Videos

* _Coming soon._
:::

:::{.callout-caution collapse="false"}

#### Exercises

To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.

* @exr-cuda

* @exr-perf
:::
